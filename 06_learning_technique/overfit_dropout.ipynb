{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.28854104084\n",
      "=== epoch:1, train acc:0.106666666667, test acc:0.1209 ===\n",
      "train loss:2.2990431869\n",
      "train loss:2.29280254054\n",
      "train loss:2.3089714897\n",
      "=== epoch:2, train acc:0.106666666667, test acc:0.123 ===\n",
      "train loss:2.29695755919\n",
      "train loss:2.29705894237\n",
      "train loss:2.28015490537\n",
      "=== epoch:3, train acc:0.106666666667, test acc:0.1267 ===\n",
      "train loss:2.2855369181\n",
      "train loss:2.29048453798\n",
      "train loss:2.29398113377\n",
      "=== epoch:4, train acc:0.126666666667, test acc:0.1325 ===\n",
      "train loss:2.28118721937\n",
      "train loss:2.29774621121\n",
      "train loss:2.30992797827\n",
      "=== epoch:5, train acc:0.133333333333, test acc:0.1346 ===\n",
      "train loss:2.29276257096\n",
      "train loss:2.27879484838\n",
      "train loss:2.3031494112\n",
      "=== epoch:6, train acc:0.14, test acc:0.1378 ===\n",
      "train loss:2.27241898969\n",
      "train loss:2.28481061655\n",
      "train loss:2.28352863585\n",
      "=== epoch:7, train acc:0.146666666667, test acc:0.1425 ===\n",
      "train loss:2.27763273065\n",
      "train loss:2.27066227476\n",
      "train loss:2.2809986835\n",
      "=== epoch:8, train acc:0.153333333333, test acc:0.1446 ===\n",
      "train loss:2.28784377349\n",
      "train loss:2.30078742424\n",
      "train loss:2.27294239156\n",
      "=== epoch:9, train acc:0.17, test acc:0.1512 ===\n",
      "train loss:2.29898292094\n",
      "train loss:2.28349571216\n",
      "train loss:2.28550033282\n",
      "=== epoch:10, train acc:0.17, test acc:0.158 ===\n",
      "train loss:2.28960178143\n",
      "train loss:2.28350465857\n",
      "train loss:2.2612004731\n",
      "=== epoch:11, train acc:0.17, test acc:0.1666 ===\n",
      "train loss:2.26572583415\n",
      "train loss:2.28387617535\n",
      "train loss:2.29135584028\n",
      "=== epoch:12, train acc:0.193333333333, test acc:0.1717 ===\n",
      "train loss:2.28294427641\n",
      "train loss:2.27739744862\n",
      "train loss:2.28048740038\n",
      "=== epoch:13, train acc:0.193333333333, test acc:0.1749 ===\n",
      "train loss:2.28091591829\n",
      "train loss:2.26429542553\n",
      "train loss:2.27421161024\n",
      "=== epoch:14, train acc:0.2, test acc:0.1778 ===\n",
      "train loss:2.28177361956\n",
      "train loss:2.28043818992\n",
      "train loss:2.2674714548\n",
      "=== epoch:15, train acc:0.223333333333, test acc:0.1833 ===\n",
      "train loss:2.28130390106\n",
      "train loss:2.27087285084\n",
      "train loss:2.27929018426\n",
      "=== epoch:16, train acc:0.23, test acc:0.1893 ===\n",
      "train loss:2.28344442747\n",
      "train loss:2.28293500407\n",
      "train loss:2.27169001077\n",
      "=== epoch:17, train acc:0.26, test acc:0.1944 ===\n",
      "train loss:2.28614999572\n",
      "train loss:2.2692903076\n",
      "train loss:2.27652660394\n",
      "=== epoch:18, train acc:0.253333333333, test acc:0.1953 ===\n",
      "train loss:2.26732751057\n",
      "train loss:2.28568420937\n",
      "train loss:2.27156877716\n",
      "=== epoch:19, train acc:0.263333333333, test acc:0.1994 ===\n",
      "train loss:2.26560118762\n",
      "train loss:2.26709872123\n",
      "train loss:2.2650799435\n",
      "=== epoch:20, train acc:0.27, test acc:0.2026 ===\n",
      "train loss:2.25499652545\n",
      "train loss:2.26848464414\n",
      "train loss:2.25827930376\n",
      "=== epoch:21, train acc:0.28, test acc:0.2047 ===\n",
      "train loss:2.25112173667\n",
      "train loss:2.26094954894\n",
      "train loss:2.2537706312\n",
      "=== epoch:22, train acc:0.293333333333, test acc:0.2093 ===\n",
      "train loss:2.27557499494\n",
      "train loss:2.25968691256\n",
      "train loss:2.2595624026\n",
      "=== epoch:23, train acc:0.303333333333, test acc:0.2123 ===\n",
      "train loss:2.27350859994\n",
      "train loss:2.25742865194\n",
      "train loss:2.25621382676\n",
      "=== epoch:24, train acc:0.303333333333, test acc:0.2171 ===\n",
      "train loss:2.26938167311\n",
      "train loss:2.25970065099\n",
      "train loss:2.26679673454\n",
      "=== epoch:25, train acc:0.3, test acc:0.2169 ===\n",
      "train loss:2.26560192963\n",
      "train loss:2.26260369134\n",
      "train loss:2.26080814358\n",
      "=== epoch:26, train acc:0.3, test acc:0.2232 ===\n",
      "train loss:2.27259350624\n",
      "train loss:2.24930570364\n",
      "train loss:2.25772779361\n",
      "=== epoch:27, train acc:0.3, test acc:0.2269 ===\n",
      "train loss:2.25196880802\n",
      "train loss:2.24396421082\n",
      "train loss:2.27449412005\n",
      "=== epoch:28, train acc:0.31, test acc:0.2277 ===\n",
      "train loss:2.26809289605\n",
      "train loss:2.24954614123\n",
      "train loss:2.24104493202\n",
      "=== epoch:29, train acc:0.31, test acc:0.2311 ===\n",
      "train loss:2.24781722723\n",
      "train loss:2.26153795318\n",
      "train loss:2.25266447211\n",
      "=== epoch:30, train acc:0.31, test acc:0.2356 ===\n",
      "train loss:2.26896739112\n",
      "train loss:2.25012437703\n",
      "train loss:2.25614954679\n",
      "=== epoch:31, train acc:0.32, test acc:0.2377 ===\n",
      "train loss:2.26419560792\n",
      "train loss:2.26722892237\n",
      "train loss:2.223237861\n",
      "=== epoch:32, train acc:0.32, test acc:0.2403 ===\n",
      "train loss:2.25554586642\n",
      "train loss:2.24139774897\n",
      "train loss:2.25515194647\n",
      "=== epoch:33, train acc:0.316666666667, test acc:0.2451 ===\n",
      "train loss:2.26608320328\n",
      "train loss:2.25656415114\n",
      "train loss:2.2655286003\n",
      "=== epoch:34, train acc:0.32, test acc:0.2474 ===\n",
      "train loss:2.2534800425\n",
      "train loss:2.25441618028\n",
      "train loss:2.24216944598\n",
      "=== epoch:35, train acc:0.326666666667, test acc:0.2484 ===\n",
      "train loss:2.25850550152\n",
      "train loss:2.25372751956\n",
      "train loss:2.26670006237\n",
      "=== epoch:36, train acc:0.33, test acc:0.2483 ===\n",
      "train loss:2.24209576361\n",
      "train loss:2.26014030319\n",
      "train loss:2.2469224408\n",
      "=== epoch:37, train acc:0.333333333333, test acc:0.2511 ===\n",
      "train loss:2.25265542889\n",
      "train loss:2.25370810948\n",
      "train loss:2.2356568978\n",
      "=== epoch:38, train acc:0.326666666667, test acc:0.2503 ===\n",
      "train loss:2.24914279044\n",
      "train loss:2.25148690911\n",
      "train loss:2.24894493923\n",
      "=== epoch:39, train acc:0.326666666667, test acc:0.2493 ===\n",
      "train loss:2.24451218323\n",
      "train loss:2.25915483241\n",
      "train loss:2.27170376213\n",
      "=== epoch:40, train acc:0.33, test acc:0.2507 ===\n",
      "train loss:2.25915961552\n",
      "train loss:2.24135048614\n",
      "train loss:2.24130748959\n",
      "=== epoch:41, train acc:0.336666666667, test acc:0.2514 ===\n",
      "train loss:2.2350045742\n",
      "train loss:2.25028076607\n",
      "train loss:2.25463645245\n",
      "=== epoch:42, train acc:0.34, test acc:0.2545 ===\n",
      "train loss:2.26348685727\n",
      "train loss:2.24525203561\n",
      "train loss:2.23862536161\n",
      "=== epoch:43, train acc:0.343333333333, test acc:0.2633 ===\n",
      "train loss:2.25173391625\n",
      "train loss:2.23953304562\n",
      "train loss:2.24639115988\n",
      "=== epoch:44, train acc:0.343333333333, test acc:0.2645 ===\n",
      "train loss:2.22804196017\n",
      "train loss:2.22533091571\n",
      "train loss:2.24974996138\n",
      "=== epoch:45, train acc:0.343333333333, test acc:0.2656 ===\n",
      "train loss:2.2554849352\n",
      "train loss:2.23476898959\n",
      "train loss:2.25278225154\n",
      "=== epoch:46, train acc:0.34, test acc:0.2686 ===\n",
      "train loss:2.23697633734\n",
      "train loss:2.22160490463\n",
      "train loss:2.2418807864\n",
      "=== epoch:47, train acc:0.34, test acc:0.2702 ===\n",
      "train loss:2.24256909068\n",
      "train loss:2.24619523253\n",
      "train loss:2.24559259286\n",
      "=== epoch:48, train acc:0.346666666667, test acc:0.2706 ===\n",
      "train loss:2.22178317587\n",
      "train loss:2.23715319076\n",
      "train loss:2.21760088895\n",
      "=== epoch:49, train acc:0.353333333333, test acc:0.2696 ===\n",
      "train loss:2.25065617856\n",
      "train loss:2.21409878707\n",
      "train loss:2.24929530577\n",
      "=== epoch:50, train acc:0.353333333333, test acc:0.2705 ===\n",
      "train loss:2.23354029485\n",
      "train loss:2.22626193635\n",
      "train loss:2.24459712593\n",
      "=== epoch:51, train acc:0.34, test acc:0.2693 ===\n",
      "train loss:2.2114235953\n",
      "train loss:2.24542346437\n",
      "train loss:2.24583911606\n",
      "=== epoch:52, train acc:0.343333333333, test acc:0.2707 ===\n",
      "train loss:2.21905751319\n",
      "train loss:2.20696318544\n",
      "train loss:2.2388260414\n",
      "=== epoch:53, train acc:0.336666666667, test acc:0.2757 ===\n",
      "train loss:2.2332444685\n",
      "train loss:2.2468267418\n",
      "train loss:2.25020205789\n",
      "=== epoch:54, train acc:0.343333333333, test acc:0.2779 ===\n",
      "train loss:2.24572974137\n",
      "train loss:2.22799034299\n",
      "train loss:2.22121038732\n",
      "=== epoch:55, train acc:0.343333333333, test acc:0.2749 ===\n",
      "train loss:2.22425138529\n",
      "train loss:2.24320835513\n",
      "train loss:2.2206430456\n",
      "=== epoch:56, train acc:0.343333333333, test acc:0.2767 ===\n",
      "train loss:2.23556288894\n",
      "train loss:2.22876017824\n",
      "train loss:2.22401749971\n",
      "=== epoch:57, train acc:0.343333333333, test acc:0.2802 ===\n",
      "train loss:2.22043528126\n",
      "train loss:2.23289532885\n",
      "train loss:2.23322727748\n",
      "=== epoch:58, train acc:0.34, test acc:0.2838 ===\n",
      "train loss:2.20841265593\n",
      "train loss:2.24176754159\n",
      "train loss:2.22114740059\n",
      "=== epoch:59, train acc:0.356666666667, test acc:0.2829 ===\n",
      "train loss:2.22244989848\n",
      "train loss:2.23093084611\n",
      "train loss:2.22124612565\n",
      "=== epoch:60, train acc:0.363333333333, test acc:0.2878 ===\n",
      "train loss:2.21240800122\n",
      "train loss:2.23887674226\n",
      "train loss:2.2326549958\n",
      "=== epoch:61, train acc:0.37, test acc:0.2906 ===\n",
      "train loss:2.20684902222\n",
      "train loss:2.20423801877\n",
      "train loss:2.23491823813\n",
      "=== epoch:62, train acc:0.37, test acc:0.29 ===\n",
      "train loss:2.20317145165\n",
      "train loss:2.23010767214\n",
      "train loss:2.21605348716\n",
      "=== epoch:63, train acc:0.373333333333, test acc:0.2883 ===\n",
      "train loss:2.20639156744\n",
      "train loss:2.21396829406\n",
      "train loss:2.22963768847\n",
      "=== epoch:64, train acc:0.363333333333, test acc:0.2857 ===\n",
      "train loss:2.21258955682\n",
      "train loss:2.22228494752\n",
      "train loss:2.20166579039\n",
      "=== epoch:65, train acc:0.363333333333, test acc:0.2842 ===\n",
      "train loss:2.21766837631\n",
      "train loss:2.22613984382\n",
      "train loss:2.20750615822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:66, train acc:0.363333333333, test acc:0.2881 ===\n",
      "train loss:2.18857114436\n",
      "train loss:2.20017025596\n",
      "train loss:2.19869957794\n",
      "=== epoch:67, train acc:0.36, test acc:0.2843 ===\n",
      "train loss:2.20949771809\n",
      "train loss:2.21282249642\n",
      "train loss:2.21540261931\n",
      "=== epoch:68, train acc:0.363333333333, test acc:0.2889 ===\n",
      "train loss:2.24212852108\n",
      "train loss:2.21152114664\n",
      "train loss:2.19670427241\n",
      "=== epoch:69, train acc:0.38, test acc:0.2962 ===\n",
      "train loss:2.18318753064\n",
      "train loss:2.21675068729\n",
      "train loss:2.22822876404\n",
      "=== epoch:70, train acc:0.383333333333, test acc:0.2958 ===\n",
      "train loss:2.2040407006\n",
      "train loss:2.18268754077\n",
      "train loss:2.19411340427\n",
      "=== epoch:71, train acc:0.383333333333, test acc:0.2992 ===\n",
      "train loss:2.22330251624\n",
      "train loss:2.16413434624\n",
      "train loss:2.18050538439\n",
      "=== epoch:72, train acc:0.38, test acc:0.2986 ===\n",
      "train loss:2.20561405154\n",
      "train loss:2.20861775551\n",
      "train loss:2.20670206574\n",
      "=== epoch:73, train acc:0.38, test acc:0.2999 ===\n",
      "train loss:2.21229445454\n",
      "train loss:2.21148446787\n",
      "train loss:2.19031825967\n",
      "=== epoch:74, train acc:0.376666666667, test acc:0.299 ===\n",
      "train loss:2.21270217025\n",
      "train loss:2.22501360689\n",
      "train loss:2.17809817756\n",
      "=== epoch:75, train acc:0.383333333333, test acc:0.2989 ===\n",
      "train loss:2.1981280991\n",
      "train loss:2.19104006439\n",
      "train loss:2.20725317379\n",
      "=== epoch:76, train acc:0.38, test acc:0.2993 ===\n",
      "train loss:2.17591940122\n",
      "train loss:2.19824606855\n",
      "train loss:2.20573800416\n",
      "=== epoch:77, train acc:0.376666666667, test acc:0.2957 ===\n",
      "train loss:2.21244063019\n",
      "train loss:2.16753443484\n",
      "train loss:2.17560023355\n",
      "=== epoch:78, train acc:0.376666666667, test acc:0.2985 ===\n",
      "train loss:2.16233334605\n",
      "train loss:2.18136363118\n",
      "train loss:2.16839529364\n",
      "=== epoch:79, train acc:0.376666666667, test acc:0.2961 ===\n",
      "train loss:2.17432546008\n",
      "train loss:2.19875157353\n",
      "train loss:2.19371965148\n",
      "=== epoch:80, train acc:0.37, test acc:0.2944 ===\n",
      "train loss:2.2037065367\n",
      "train loss:2.18669929809\n",
      "train loss:2.16835872664\n",
      "=== epoch:81, train acc:0.38, test acc:0.2953 ===\n",
      "train loss:2.17643185614\n",
      "train loss:2.19202534512\n",
      "train loss:2.19011940673\n",
      "=== epoch:82, train acc:0.38, test acc:0.2994 ===\n",
      "train loss:2.17280652449\n",
      "train loss:2.19893931457\n",
      "train loss:2.18125004941\n",
      "=== epoch:83, train acc:0.38, test acc:0.3 ===\n",
      "train loss:2.16183061516\n",
      "train loss:2.16418633617\n",
      "train loss:2.1749256049\n",
      "=== epoch:84, train acc:0.383333333333, test acc:0.3042 ===\n",
      "train loss:2.14842452445\n",
      "train loss:2.16248199451\n",
      "train loss:2.19662607354\n",
      "=== epoch:85, train acc:0.386666666667, test acc:0.3057 ===\n",
      "train loss:2.16428338562\n",
      "train loss:2.19110660069\n",
      "train loss:2.18152985535\n",
      "=== epoch:86, train acc:0.39, test acc:0.3109 ===\n",
      "train loss:2.13648224489\n",
      "train loss:2.16369921058\n",
      "train loss:2.14737145997\n",
      "=== epoch:87, train acc:0.386666666667, test acc:0.309 ===\n",
      "train loss:2.19609270105\n",
      "train loss:2.15393555098\n",
      "train loss:2.1811986745\n",
      "=== epoch:88, train acc:0.39, test acc:0.3118 ===\n",
      "train loss:2.18615961101\n",
      "train loss:2.15848570549\n",
      "train loss:2.17414980009\n",
      "=== epoch:89, train acc:0.393333333333, test acc:0.3151 ===\n",
      "train loss:2.17165463679\n",
      "train loss:2.15703356059\n",
      "train loss:2.19498751569\n",
      "=== epoch:90, train acc:0.39, test acc:0.3152 ===\n",
      "train loss:2.18373649866\n",
      "train loss:2.14342198407\n",
      "train loss:2.14852141572\n",
      "=== epoch:91, train acc:0.4, test acc:0.3158 ===\n",
      "train loss:2.17466692169\n",
      "train loss:2.17784232521\n",
      "train loss:2.15454244608\n",
      "=== epoch:92, train acc:0.396666666667, test acc:0.3171 ===\n",
      "train loss:2.18231551865\n",
      "train loss:2.17009552479\n",
      "train loss:2.14703990995\n",
      "=== epoch:93, train acc:0.4, test acc:0.3175 ===\n",
      "train loss:2.14428163385\n",
      "train loss:2.2003581918\n",
      "train loss:2.13443309277\n",
      "=== epoch:94, train acc:0.403333333333, test acc:0.3166 ===\n",
      "train loss:2.18180702784\n",
      "train loss:2.12542804575\n",
      "train loss:2.13522512999\n",
      "=== epoch:95, train acc:0.41, test acc:0.3186 ===\n",
      "train loss:2.13824842698\n",
      "train loss:2.14113298856\n",
      "train loss:2.1720310642\n",
      "=== epoch:96, train acc:0.403333333333, test acc:0.3147 ===\n",
      "train loss:2.16925052545\n",
      "train loss:2.17698226269\n",
      "train loss:2.12631245256\n",
      "=== epoch:97, train acc:0.403333333333, test acc:0.3145 ===\n",
      "train loss:2.15766240731\n",
      "train loss:2.16170354598\n",
      "train loss:2.14601762051\n",
      "=== epoch:98, train acc:0.396666666667, test acc:0.3152 ===\n",
      "train loss:2.13828010671\n",
      "train loss:2.12030231342\n",
      "train loss:2.15856184561\n",
      "=== epoch:99, train acc:0.403333333333, test acc:0.3159 ===\n",
      "train loss:2.14070863394\n",
      "train loss:2.18619826116\n",
      "train loss:2.12190653293\n",
      "=== epoch:100, train acc:0.403333333333, test acc:0.3191 ===\n",
      "train loss:2.1206716556\n",
      "train loss:2.16450182972\n",
      "train loss:2.15826344639\n",
      "=== epoch:101, train acc:0.406666666667, test acc:0.3206 ===\n",
      "train loss:2.1376976709\n",
      "train loss:2.16294865882\n",
      "train loss:2.12761526354\n",
      "=== epoch:102, train acc:0.41, test acc:0.3254 ===\n",
      "train loss:2.10315887486\n",
      "train loss:2.17575544535\n",
      "train loss:2.12160334673\n",
      "=== epoch:103, train acc:0.403333333333, test acc:0.3235 ===\n",
      "train loss:2.14989853588\n",
      "train loss:2.12535141867\n",
      "train loss:2.11343255491\n",
      "=== epoch:104, train acc:0.4, test acc:0.3202 ===\n",
      "train loss:2.16554221081\n",
      "train loss:2.14654917469\n",
      "train loss:2.09543205718\n",
      "=== epoch:105, train acc:0.403333333333, test acc:0.3153 ===\n",
      "train loss:2.11695089145\n",
      "train loss:2.11400970194\n",
      "train loss:2.12526295074\n",
      "=== epoch:106, train acc:0.4, test acc:0.3137 ===\n",
      "train loss:2.15707228107\n",
      "train loss:2.09514018344\n",
      "train loss:2.1396067692\n",
      "=== epoch:107, train acc:0.4, test acc:0.3108 ===\n",
      "train loss:2.14705199144\n",
      "train loss:2.13361305524\n",
      "train loss:2.12635854924\n",
      "=== epoch:108, train acc:0.4, test acc:0.3161 ===\n",
      "train loss:2.12785417904\n",
      "train loss:2.09872773078\n",
      "train loss:2.152386188\n",
      "=== epoch:109, train acc:0.403333333333, test acc:0.3192 ===\n",
      "train loss:2.10805005141\n",
      "train loss:2.12328294893\n",
      "train loss:2.10939446689\n",
      "=== epoch:110, train acc:0.403333333333, test acc:0.32 ===\n",
      "train loss:2.12753273121\n",
      "train loss:2.08095207619\n",
      "train loss:2.14283130679\n",
      "=== epoch:111, train acc:0.403333333333, test acc:0.3199 ===\n",
      "train loss:2.16307421389\n",
      "train loss:2.1264245528\n",
      "train loss:2.12407600677\n",
      "=== epoch:112, train acc:0.403333333333, test acc:0.3213 ===\n",
      "train loss:2.07925451715\n",
      "train loss:2.11632708832\n",
      "train loss:2.11789320208\n",
      "=== epoch:113, train acc:0.406666666667, test acc:0.3226 ===\n",
      "train loss:2.11018199659\n",
      "train loss:2.06900969033\n",
      "train loss:2.07887062808\n",
      "=== epoch:114, train acc:0.413333333333, test acc:0.3268 ===\n",
      "train loss:2.11854852139\n",
      "train loss:2.12209778999\n",
      "train loss:2.13502495088\n",
      "=== epoch:115, train acc:0.416666666667, test acc:0.3305 ===\n",
      "train loss:2.10979396397\n",
      "train loss:2.08486204605\n",
      "train loss:2.12336712432\n",
      "=== epoch:116, train acc:0.413333333333, test acc:0.3302 ===\n",
      "train loss:2.108259347\n",
      "train loss:2.09198735918\n",
      "train loss:2.12614053923\n",
      "=== epoch:117, train acc:0.413333333333, test acc:0.3321 ===\n",
      "train loss:2.09337637213\n",
      "train loss:2.1140839216\n",
      "train loss:2.08919218262\n",
      "=== epoch:118, train acc:0.42, test acc:0.3364 ===\n",
      "train loss:2.10004748817\n",
      "train loss:2.06989731859\n",
      "train loss:2.11188942364\n",
      "=== epoch:119, train acc:0.43, test acc:0.3409 ===\n",
      "train loss:2.06615939806\n",
      "train loss:2.07341024047\n",
      "train loss:2.07303552036\n",
      "=== epoch:120, train acc:0.426666666667, test acc:0.3388 ===\n",
      "train loss:2.11927097976\n",
      "train loss:2.08332529258\n",
      "train loss:2.11500221418\n",
      "=== epoch:121, train acc:0.436666666667, test acc:0.3413 ===\n",
      "train loss:2.14137476467\n",
      "train loss:2.08871288732\n",
      "train loss:2.12803711991\n",
      "=== epoch:122, train acc:0.44, test acc:0.3409 ===\n",
      "train loss:2.06446998895\n",
      "train loss:2.05509589394\n",
      "train loss:2.07841547136\n",
      "=== epoch:123, train acc:0.436666666667, test acc:0.3409 ===\n",
      "train loss:2.09309387572\n",
      "train loss:2.08931824605\n",
      "train loss:2.0574258019\n",
      "=== epoch:124, train acc:0.436666666667, test acc:0.343 ===\n",
      "train loss:2.07884650514\n",
      "train loss:2.04607761279\n",
      "train loss:2.09182095069\n",
      "=== epoch:125, train acc:0.446666666667, test acc:0.346 ===\n",
      "train loss:2.05301445186\n",
      "train loss:2.07818319393\n",
      "train loss:2.06980460701\n",
      "=== epoch:126, train acc:0.46, test acc:0.3511 ===\n",
      "train loss:1.97545448775\n",
      "train loss:2.10523574147\n",
      "train loss:2.04960709636\n",
      "=== epoch:127, train acc:0.456666666667, test acc:0.3525 ===\n",
      "train loss:2.04080722219\n",
      "train loss:2.0740450336\n",
      "train loss:2.08816719642\n",
      "=== epoch:128, train acc:0.456666666667, test acc:0.3526 ===\n",
      "train loss:2.05966079143\n",
      "train loss:2.05094951463\n",
      "train loss:2.04168160665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:129, train acc:0.456666666667, test acc:0.3543 ===\n",
      "train loss:2.05245660852\n",
      "train loss:2.01741586571\n",
      "train loss:2.0373030059\n",
      "=== epoch:130, train acc:0.456666666667, test acc:0.357 ===\n",
      "train loss:2.05177527864\n",
      "train loss:2.02782079396\n",
      "train loss:1.99655523299\n",
      "=== epoch:131, train acc:0.466666666667, test acc:0.3612 ===\n",
      "train loss:2.05512040953\n",
      "train loss:2.02252707037\n",
      "train loss:2.1069542971\n",
      "=== epoch:132, train acc:0.47, test acc:0.3628 ===\n",
      "train loss:2.03710628986\n",
      "train loss:2.0461325211\n",
      "train loss:2.09490007831\n",
      "=== epoch:133, train acc:0.466666666667, test acc:0.3623 ===\n",
      "train loss:2.05621188538\n",
      "train loss:2.00104596379\n",
      "train loss:2.07831199183\n",
      "=== epoch:134, train acc:0.47, test acc:0.3634 ===\n",
      "train loss:2.03025928426\n",
      "train loss:2.04298225288\n",
      "train loss:2.04694602705\n",
      "=== epoch:135, train acc:0.473333333333, test acc:0.3663 ===\n",
      "train loss:2.07867314234\n",
      "train loss:2.03454290639\n",
      "train loss:1.96504958345\n",
      "=== epoch:136, train acc:0.47, test acc:0.3707 ===\n",
      "train loss:1.98666203829\n",
      "train loss:2.01563581718\n",
      "train loss:2.06252588386\n",
      "=== epoch:137, train acc:0.47, test acc:0.3729 ===\n",
      "train loss:2.01681636018\n",
      "train loss:1.95126646915\n",
      "train loss:2.0132593135\n",
      "=== epoch:138, train acc:0.47, test acc:0.3687 ===\n",
      "train loss:1.97205902109\n",
      "train loss:2.05613907833\n",
      "train loss:2.0388918369\n",
      "=== epoch:139, train acc:0.476666666667, test acc:0.3674 ===\n",
      "train loss:2.03715863592\n",
      "train loss:1.94262880557\n",
      "train loss:2.04307327252\n",
      "=== epoch:140, train acc:0.47, test acc:0.3718 ===\n",
      "train loss:2.03604183333\n",
      "train loss:1.9628779635\n",
      "train loss:1.98345810432\n",
      "=== epoch:141, train acc:0.476666666667, test acc:0.3711 ===\n",
      "train loss:2.0003509375\n",
      "train loss:2.05980184179\n",
      "train loss:1.96894360361\n",
      "=== epoch:142, train acc:0.473333333333, test acc:0.3729 ===\n",
      "train loss:2.04977737984\n",
      "train loss:2.00615517697\n",
      "train loss:2.01891504545\n",
      "=== epoch:143, train acc:0.483333333333, test acc:0.3794 ===\n",
      "train loss:1.93158065403\n",
      "train loss:1.90647203797\n",
      "train loss:1.908242484\n",
      "=== epoch:144, train acc:0.47, test acc:0.3725 ===\n",
      "train loss:2.03984746098\n",
      "train loss:1.94531262821\n",
      "train loss:1.92708455454\n",
      "=== epoch:145, train acc:0.476666666667, test acc:0.3794 ===\n",
      "train loss:1.89798806511\n",
      "train loss:1.89550623471\n",
      "train loss:1.93977050215\n",
      "=== epoch:146, train acc:0.47, test acc:0.3765 ===\n",
      "train loss:1.97366501429\n",
      "train loss:1.99693011098\n",
      "train loss:1.95323365293\n",
      "=== epoch:147, train acc:0.47, test acc:0.3809 ===\n",
      "train loss:2.00856979932\n",
      "train loss:1.98084210736\n",
      "train loss:1.94745662051\n",
      "=== epoch:148, train acc:0.483333333333, test acc:0.3852 ===\n",
      "train loss:1.90341836857\n",
      "train loss:1.98896063112\n",
      "train loss:1.91276428648\n",
      "=== epoch:149, train acc:0.476666666667, test acc:0.3864 ===\n",
      "train loss:1.94176471789\n",
      "train loss:1.90575428185\n",
      "train loss:1.95416737427\n",
      "=== epoch:150, train acc:0.47, test acc:0.3857 ===\n",
      "train loss:1.97773793122\n",
      "train loss:2.03903520836\n",
      "train loss:2.0218996192\n",
      "=== epoch:151, train acc:0.486666666667, test acc:0.3908 ===\n",
      "train loss:1.91918656175\n",
      "train loss:1.97863721912\n",
      "train loss:1.93117122074\n",
      "=== epoch:152, train acc:0.483333333333, test acc:0.3889 ===\n",
      "train loss:1.96818283689\n",
      "train loss:1.90850960502\n",
      "train loss:1.86659297632\n",
      "=== epoch:153, train acc:0.483333333333, test acc:0.3915 ===\n",
      "train loss:1.93245468648\n",
      "train loss:1.97679969646\n",
      "train loss:1.86095507823\n",
      "=== epoch:154, train acc:0.486666666667, test acc:0.3947 ===\n",
      "train loss:1.94040808793\n",
      "train loss:1.87573771562\n",
      "train loss:1.8655293465\n",
      "=== epoch:155, train acc:0.493333333333, test acc:0.3962 ===\n",
      "train loss:1.86096521862\n",
      "train loss:1.90257659677\n",
      "train loss:1.92239507283\n",
      "=== epoch:156, train acc:0.496666666667, test acc:0.3958 ===\n",
      "train loss:1.92526160731\n",
      "train loss:1.86123602009\n",
      "train loss:1.81498303932\n",
      "=== epoch:157, train acc:0.496666666667, test acc:0.3923 ===\n",
      "train loss:1.84844586383\n",
      "train loss:1.85311186083\n",
      "train loss:1.86285241421\n",
      "=== epoch:158, train acc:0.49, test acc:0.3962 ===\n",
      "train loss:1.86542138987\n",
      "train loss:1.94627303812\n",
      "train loss:1.92430255915\n",
      "=== epoch:159, train acc:0.483333333333, test acc:0.3943 ===\n",
      "train loss:1.96371611801\n",
      "train loss:1.90489736009\n",
      "train loss:1.92265077124\n",
      "=== epoch:160, train acc:0.49, test acc:0.3955 ===\n",
      "train loss:1.89444253821\n",
      "train loss:1.91965354067\n",
      "train loss:1.91293173279\n",
      "=== epoch:161, train acc:0.486666666667, test acc:0.399 ===\n",
      "train loss:1.97225491601\n",
      "train loss:1.80435907834\n",
      "train loss:1.8566519219\n",
      "=== epoch:162, train acc:0.49, test acc:0.4018 ===\n",
      "train loss:1.92179224403\n",
      "train loss:1.91460592119\n",
      "train loss:1.80852318515\n",
      "=== epoch:163, train acc:0.493333333333, test acc:0.402 ===\n",
      "train loss:1.94056102961\n",
      "train loss:1.882129087\n",
      "train loss:1.85141554383\n",
      "=== epoch:164, train acc:0.493333333333, test acc:0.4057 ===\n",
      "train loss:1.89732350633\n",
      "train loss:1.81560507663\n",
      "train loss:1.81801329775\n",
      "=== epoch:165, train acc:0.503333333333, test acc:0.4105 ===\n",
      "train loss:1.84505329743\n",
      "train loss:1.84590019278\n",
      "train loss:1.83400328741\n",
      "=== epoch:166, train acc:0.503333333333, test acc:0.4097 ===\n",
      "train loss:1.76303720611\n",
      "train loss:1.76966241697\n",
      "train loss:1.93152723207\n",
      "=== epoch:167, train acc:0.503333333333, test acc:0.41 ===\n",
      "train loss:1.86739798262\n",
      "train loss:1.78286872964\n",
      "train loss:1.81774884561\n",
      "=== epoch:168, train acc:0.5, test acc:0.4113 ===\n",
      "train loss:1.8297186457\n",
      "train loss:1.834426928\n",
      "train loss:1.80574328442\n",
      "=== epoch:169, train acc:0.5, test acc:0.4105 ===\n",
      "train loss:1.82691060605\n",
      "train loss:1.74983692159\n",
      "train loss:1.76716412894\n",
      "=== epoch:170, train acc:0.5, test acc:0.4113 ===\n",
      "train loss:1.89322658222\n",
      "train loss:1.7953340689\n",
      "train loss:1.86797245585\n",
      "=== epoch:171, train acc:0.496666666667, test acc:0.4106 ===\n",
      "train loss:1.77696862812\n",
      "train loss:1.7787740449\n",
      "train loss:1.81561297444\n",
      "=== epoch:172, train acc:0.493333333333, test acc:0.4129 ===\n",
      "train loss:1.82111358689\n",
      "train loss:1.75983310902\n",
      "train loss:1.81493861282\n",
      "=== epoch:173, train acc:0.5, test acc:0.4156 ===\n",
      "train loss:1.86827613583\n",
      "train loss:1.80959799947\n",
      "train loss:1.82823805997\n",
      "=== epoch:174, train acc:0.503333333333, test acc:0.4177 ===\n",
      "train loss:1.62916916764\n",
      "train loss:1.75260788377\n",
      "train loss:1.78097909555\n",
      "=== epoch:175, train acc:0.503333333333, test acc:0.4184 ===\n",
      "train loss:1.7221050841\n",
      "train loss:1.77911060488\n",
      "train loss:1.73519946445\n",
      "=== epoch:176, train acc:0.503333333333, test acc:0.4184 ===\n",
      "train loss:1.72211258663\n",
      "train loss:1.82981653251\n",
      "train loss:1.83645607621\n",
      "=== epoch:177, train acc:0.5, test acc:0.4195 ===\n",
      "train loss:1.66088522016\n",
      "train loss:1.78494626394\n",
      "train loss:1.78963246004\n",
      "=== epoch:178, train acc:0.496666666667, test acc:0.421 ===\n",
      "train loss:1.78322215901\n",
      "train loss:1.74940150482\n",
      "train loss:1.71020123783\n",
      "=== epoch:179, train acc:0.5, test acc:0.422 ===\n",
      "train loss:1.81983632245\n",
      "train loss:1.64968660095\n",
      "train loss:1.74614045134\n",
      "=== epoch:180, train acc:0.503333333333, test acc:0.4244 ===\n",
      "train loss:1.73092193934\n",
      "train loss:1.71676715645\n",
      "train loss:1.68538891224\n",
      "=== epoch:181, train acc:0.5, test acc:0.4249 ===\n",
      "train loss:1.67703469643\n",
      "train loss:1.70037557563\n",
      "train loss:1.75127153095\n",
      "=== epoch:182, train acc:0.5, test acc:0.4246 ===\n",
      "train loss:1.72354746825\n",
      "train loss:1.71076374187\n",
      "train loss:1.63789246859\n",
      "=== epoch:183, train acc:0.506666666667, test acc:0.4227 ===\n",
      "train loss:1.80742936167\n",
      "train loss:1.63298167737\n",
      "train loss:1.75546461212\n",
      "=== epoch:184, train acc:0.506666666667, test acc:0.4274 ===\n",
      "train loss:1.69807324255\n",
      "train loss:1.67885952239\n",
      "train loss:1.76240123349\n",
      "=== epoch:185, train acc:0.506666666667, test acc:0.4264 ===\n",
      "train loss:1.75309765521\n",
      "train loss:1.72495390824\n",
      "train loss:1.74399501318\n",
      "=== epoch:186, train acc:0.503333333333, test acc:0.4265 ===\n",
      "train loss:1.60686116807\n",
      "train loss:1.7255734543\n",
      "train loss:1.48529786256\n",
      "=== epoch:187, train acc:0.51, test acc:0.426 ===\n",
      "train loss:1.6839255899\n",
      "train loss:1.71268958165\n",
      "train loss:1.60203371605\n",
      "=== epoch:188, train acc:0.506666666667, test acc:0.4274 ===\n",
      "train loss:1.70366593429\n",
      "train loss:1.65587156624\n",
      "train loss:1.70775995669\n",
      "=== epoch:189, train acc:0.503333333333, test acc:0.4285 ===\n",
      "train loss:1.63357661219\n",
      "train loss:1.67753986433\n",
      "train loss:1.57428995814\n",
      "=== epoch:190, train acc:0.506666666667, test acc:0.4295 ===\n",
      "train loss:1.70282565056\n",
      "train loss:1.59154240535\n",
      "train loss:1.66960551843\n",
      "=== epoch:191, train acc:0.51, test acc:0.4306 ===\n",
      "train loss:1.70498992903\n",
      "train loss:1.66571609001\n",
      "train loss:1.68583904604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:192, train acc:0.513333333333, test acc:0.4338 ===\n",
      "train loss:1.78984030563\n",
      "train loss:1.64489273883\n",
      "train loss:1.72085070916\n",
      "=== epoch:193, train acc:0.513333333333, test acc:0.4371 ===\n",
      "train loss:1.73164271541\n",
      "train loss:1.5702734138\n",
      "train loss:1.66800624541\n",
      "=== epoch:194, train acc:0.52, test acc:0.4372 ===\n",
      "train loss:1.56221512879\n",
      "train loss:1.69289788758\n",
      "train loss:1.63873636966\n",
      "=== epoch:195, train acc:0.513333333333, test acc:0.4359 ===\n",
      "train loss:1.67662283732\n",
      "train loss:1.64501901677\n",
      "train loss:1.65073023473\n",
      "=== epoch:196, train acc:0.513333333333, test acc:0.4346 ===\n",
      "train loss:1.59171357362\n",
      "train loss:1.66085429362\n",
      "train loss:1.52547900467\n",
      "=== epoch:197, train acc:0.51, test acc:0.4352 ===\n",
      "train loss:1.59871534323\n",
      "train loss:1.70886527272\n",
      "train loss:1.63553337186\n",
      "=== epoch:198, train acc:0.513333333333, test acc:0.4359 ===\n",
      "train loss:1.58985283253\n",
      "train loss:1.49723550098\n",
      "train loss:1.63450043548\n",
      "=== epoch:199, train acc:0.513333333333, test acc:0.435 ===\n",
      "train loss:1.65837154173\n",
      "train loss:1.6034393113\n",
      "train loss:1.49163985246\n",
      "=== epoch:200, train acc:0.513333333333, test acc:0.4347 ===\n",
      "train loss:1.53806375404\n",
      "train loss:1.57060315279\n",
      "train loss:1.58233304862\n",
      "=== epoch:201, train acc:0.52, test acc:0.4397 ===\n",
      "train loss:1.77295691391\n",
      "train loss:1.57983674684\n",
      "train loss:1.53193007903\n",
      "=== epoch:202, train acc:0.523333333333, test acc:0.4421 ===\n",
      "train loss:1.6143764537\n",
      "train loss:1.69927096317\n",
      "train loss:1.52657940202\n",
      "=== epoch:203, train acc:0.533333333333, test acc:0.4433 ===\n",
      "train loss:1.6064505065\n",
      "train loss:1.58416703972\n",
      "train loss:1.54837299841\n",
      "=== epoch:204, train acc:0.536666666667, test acc:0.4448 ===\n",
      "train loss:1.58120385859\n",
      "train loss:1.55378446224\n",
      "train loss:1.55140743732\n",
      "=== epoch:205, train acc:0.533333333333, test acc:0.447 ===\n",
      "train loss:1.44638065245\n",
      "train loss:1.54989072316\n",
      "train loss:1.48565903354\n",
      "=== epoch:206, train acc:0.53, test acc:0.4441 ===\n",
      "train loss:1.56435574819\n",
      "train loss:1.48524063255\n",
      "train loss:1.50458288754\n",
      "=== epoch:207, train acc:0.536666666667, test acc:0.4467 ===\n",
      "train loss:1.54725962187\n",
      "train loss:1.46582055839\n",
      "train loss:1.7306718393\n",
      "=== epoch:208, train acc:0.536666666667, test acc:0.4518 ===\n",
      "train loss:1.55972962183\n",
      "train loss:1.55485230799\n",
      "train loss:1.34225448762\n",
      "=== epoch:209, train acc:0.53, test acc:0.4501 ===\n",
      "train loss:1.61944407329\n",
      "train loss:1.54378039172\n",
      "train loss:1.53611500073\n",
      "=== epoch:210, train acc:0.53, test acc:0.4517 ===\n",
      "train loss:1.6492673189\n",
      "train loss:1.59028236869\n",
      "train loss:1.48281703163\n",
      "=== epoch:211, train acc:0.53, test acc:0.4523 ===\n",
      "train loss:1.45813014596\n",
      "train loss:1.64462202333\n",
      "train loss:1.50690999956\n",
      "=== epoch:212, train acc:0.53, test acc:0.4546 ===\n",
      "train loss:1.50631022952\n",
      "train loss:1.41552981915\n",
      "train loss:1.57024064688\n",
      "=== epoch:213, train acc:0.536666666667, test acc:0.4563 ===\n",
      "train loss:1.54326103403\n",
      "train loss:1.48302376511\n",
      "train loss:1.56893597443\n",
      "=== epoch:214, train acc:0.533333333333, test acc:0.4535 ===\n",
      "train loss:1.57924889064\n",
      "train loss:1.51792170864\n",
      "train loss:1.4861763717\n",
      "=== epoch:215, train acc:0.536666666667, test acc:0.4543 ===\n",
      "train loss:1.51254554306\n",
      "train loss:1.45337262361\n",
      "train loss:1.62631604073\n",
      "=== epoch:216, train acc:0.536666666667, test acc:0.4536 ===\n",
      "train loss:1.43895537442\n",
      "train loss:1.50897724517\n",
      "train loss:1.49134060622\n",
      "=== epoch:217, train acc:0.54, test acc:0.4529 ===\n",
      "train loss:1.5783097385\n",
      "train loss:1.44228996507\n",
      "train loss:1.47345043607\n",
      "=== epoch:218, train acc:0.54, test acc:0.453 ===\n",
      "train loss:1.45004151479\n",
      "train loss:1.39941717533\n",
      "train loss:1.58131891418\n",
      "=== epoch:219, train acc:0.54, test acc:0.4549 ===\n",
      "train loss:1.5259021123\n",
      "train loss:1.41973110654\n",
      "train loss:1.36619010797\n",
      "=== epoch:220, train acc:0.54, test acc:0.4555 ===\n",
      "train loss:1.39152388403\n",
      "train loss:1.40734497696\n",
      "train loss:1.57464638874\n",
      "=== epoch:221, train acc:0.546666666667, test acc:0.4575 ===\n",
      "train loss:1.33792368547\n",
      "train loss:1.45355308765\n",
      "train loss:1.62343313663\n",
      "=== epoch:222, train acc:0.546666666667, test acc:0.4623 ===\n",
      "train loss:1.50099696925\n",
      "train loss:1.36958901143\n",
      "train loss:1.40018948396\n",
      "=== epoch:223, train acc:0.556666666667, test acc:0.4669 ===\n",
      "train loss:1.33786050854\n",
      "train loss:1.48308990722\n",
      "train loss:1.32535997982\n",
      "=== epoch:224, train acc:0.556666666667, test acc:0.4679 ===\n",
      "train loss:1.31769812336\n",
      "train loss:1.42428340808\n",
      "train loss:1.39791667975\n",
      "=== epoch:225, train acc:0.553333333333, test acc:0.4693 ===\n",
      "train loss:1.45663831991\n",
      "train loss:1.37238556368\n",
      "train loss:1.41912661716\n",
      "=== epoch:226, train acc:0.556666666667, test acc:0.4701 ===\n",
      "train loss:1.5366541756\n",
      "train loss:1.34063514068\n",
      "train loss:1.33122714656\n",
      "=== epoch:227, train acc:0.56, test acc:0.4708 ===\n",
      "train loss:1.40379565469\n",
      "train loss:1.44119547987\n",
      "train loss:1.49138559542\n",
      "=== epoch:228, train acc:0.56, test acc:0.4737 ===\n",
      "train loss:1.44030504394\n",
      "train loss:1.41081865165\n",
      "train loss:1.39762866683\n",
      "=== epoch:229, train acc:0.56, test acc:0.4714 ===\n",
      "train loss:1.38730660715\n",
      "train loss:1.42287064405\n",
      "train loss:1.53764229643\n",
      "=== epoch:230, train acc:0.58, test acc:0.4788 ===\n",
      "train loss:1.39019383251\n",
      "train loss:1.25393155509\n",
      "train loss:1.4543658762\n",
      "=== epoch:231, train acc:0.58, test acc:0.4793 ===\n",
      "train loss:1.36908368231\n",
      "train loss:1.4369395241\n",
      "train loss:1.51029656448\n",
      "=== epoch:232, train acc:0.573333333333, test acc:0.48 ===\n",
      "train loss:1.36072875226\n",
      "train loss:1.37423499041\n",
      "train loss:1.4173168129\n",
      "=== epoch:233, train acc:0.566666666667, test acc:0.4802 ===\n",
      "train loss:1.33824064234\n",
      "train loss:1.43959810694\n",
      "train loss:1.31886038685\n",
      "=== epoch:234, train acc:0.566666666667, test acc:0.4773 ===\n",
      "train loss:1.32375951142\n",
      "train loss:1.39928461087\n",
      "train loss:1.40452428498\n",
      "=== epoch:235, train acc:0.566666666667, test acc:0.4788 ===\n",
      "train loss:1.30010769987\n",
      "train loss:1.3529377498\n",
      "train loss:1.44805811476\n",
      "=== epoch:236, train acc:0.563333333333, test acc:0.4801 ===\n",
      "train loss:1.33706128276\n",
      "train loss:1.32129670717\n",
      "train loss:1.29642884947\n",
      "=== epoch:237, train acc:0.57, test acc:0.4798 ===\n",
      "train loss:1.33055190005\n",
      "train loss:1.28740622897\n",
      "train loss:1.30511813554\n",
      "=== epoch:238, train acc:0.566666666667, test acc:0.4793 ===\n",
      "train loss:1.33431581805\n",
      "train loss:1.38836768206\n",
      "train loss:1.37016672356\n",
      "=== epoch:239, train acc:0.573333333333, test acc:0.4807 ===\n",
      "train loss:1.29512369705\n",
      "train loss:1.36572216688\n",
      "train loss:1.47959758725\n",
      "=== epoch:240, train acc:0.58, test acc:0.4854 ===\n",
      "train loss:1.36770488888\n",
      "train loss:1.38625352417\n",
      "train loss:1.37189175669\n",
      "=== epoch:241, train acc:0.583333333333, test acc:0.4852 ===\n",
      "train loss:1.31904542773\n",
      "train loss:1.39416030782\n",
      "train loss:1.31237043274\n",
      "=== epoch:242, train acc:0.58, test acc:0.4861 ===\n",
      "train loss:1.27263964537\n",
      "train loss:1.34113067959\n",
      "train loss:1.45865324927\n",
      "=== epoch:243, train acc:0.586666666667, test acc:0.4887 ===\n",
      "train loss:1.25181102203\n",
      "train loss:1.29853262384\n",
      "train loss:1.26708406773\n",
      "=== epoch:244, train acc:0.583333333333, test acc:0.4883 ===\n",
      "train loss:1.17991347706\n",
      "train loss:1.30483220604\n",
      "train loss:1.28078359075\n",
      "=== epoch:245, train acc:0.576666666667, test acc:0.4884 ===\n",
      "train loss:1.34939297322\n",
      "train loss:1.29401294997\n",
      "train loss:1.40821739701\n",
      "=== epoch:246, train acc:0.586666666667, test acc:0.4943 ===\n",
      "train loss:1.45205606223\n",
      "train loss:1.23935520376\n",
      "train loss:1.37007676933\n",
      "=== epoch:247, train acc:0.596666666667, test acc:0.4946 ===\n",
      "train loss:1.37834083599\n",
      "train loss:1.37077032439\n",
      "train loss:1.18311636633\n",
      "=== epoch:248, train acc:0.593333333333, test acc:0.4979 ===\n",
      "train loss:1.34673520717\n",
      "train loss:1.24059410565\n",
      "train loss:1.20921091151\n",
      "=== epoch:249, train acc:0.596666666667, test acc:0.5014 ===\n",
      "train loss:1.3153353754\n",
      "train loss:1.25013908381\n",
      "train loss:1.26022705366\n",
      "=== epoch:250, train acc:0.596666666667, test acc:0.4964 ===\n",
      "train loss:1.36796674004\n",
      "train loss:1.27974195509\n",
      "train loss:1.406181219\n",
      "=== epoch:251, train acc:0.6, test acc:0.5003 ===\n",
      "train loss:1.27288006682\n",
      "train loss:1.19168675619\n",
      "train loss:1.3138429302\n",
      "=== epoch:252, train acc:0.6, test acc:0.5014 ===\n",
      "train loss:1.369349749\n",
      "train loss:1.18659924194\n",
      "train loss:1.34658196054\n",
      "=== epoch:253, train acc:0.603333333333, test acc:0.5007 ===\n",
      "train loss:1.28173153764\n",
      "train loss:1.26750465808\n",
      "train loss:1.21177202617\n",
      "=== epoch:254, train acc:0.603333333333, test acc:0.502 ===\n",
      "train loss:1.28934830643\n",
      "train loss:1.27188572248\n",
      "train loss:1.21601337482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:255, train acc:0.606666666667, test acc:0.5047 ===\n",
      "train loss:1.28172030287\n",
      "train loss:1.21551548054\n",
      "train loss:1.34130444413\n",
      "=== epoch:256, train acc:0.613333333333, test acc:0.5122 ===\n",
      "train loss:1.13951638364\n",
      "train loss:1.17207819322\n",
      "train loss:1.11349002463\n",
      "=== epoch:257, train acc:0.62, test acc:0.5093 ===\n",
      "train loss:1.35710281556\n",
      "train loss:1.28950561908\n",
      "train loss:1.31234826569\n",
      "=== epoch:258, train acc:0.62, test acc:0.5116 ===\n",
      "train loss:1.28247693403\n",
      "train loss:1.34460241967\n",
      "train loss:1.17436016006\n",
      "=== epoch:259, train acc:0.613333333333, test acc:0.5125 ===\n",
      "train loss:1.00808646004\n",
      "train loss:1.15463642303\n",
      "train loss:1.10290534412\n",
      "=== epoch:260, train acc:0.616666666667, test acc:0.5147 ===\n",
      "train loss:1.32721619482\n",
      "train loss:1.15850178961\n",
      "train loss:1.18932550949\n",
      "=== epoch:261, train acc:0.626666666667, test acc:0.5163 ===\n",
      "train loss:1.11207062133\n",
      "train loss:1.21176921647\n",
      "train loss:1.03029212451\n",
      "=== epoch:262, train acc:0.626666666667, test acc:0.5145 ===\n",
      "train loss:1.16970524506\n",
      "train loss:1.30303831753\n",
      "train loss:1.29256032335\n",
      "=== epoch:263, train acc:0.62, test acc:0.5189 ===\n",
      "train loss:1.1834484048\n",
      "train loss:1.19825579239\n",
      "train loss:1.30081488517\n",
      "=== epoch:264, train acc:0.63, test acc:0.5203 ===\n",
      "train loss:1.22883216417\n",
      "train loss:1.15480193447\n",
      "train loss:1.11996858966\n",
      "=== epoch:265, train acc:0.616666666667, test acc:0.5177 ===\n",
      "train loss:1.03481777284\n",
      "train loss:1.12290677236\n",
      "train loss:1.21469896866\n",
      "=== epoch:266, train acc:0.626666666667, test acc:0.516 ===\n",
      "train loss:1.13773561302\n",
      "train loss:1.12379014033\n",
      "train loss:1.14008976348\n",
      "=== epoch:267, train acc:0.616666666667, test acc:0.5178 ===\n",
      "train loss:1.03106225755\n",
      "train loss:1.05523205483\n",
      "train loss:1.36649343561\n",
      "=== epoch:268, train acc:0.633333333333, test acc:0.5209 ===\n",
      "train loss:1.09778592797\n",
      "train loss:1.22036830387\n",
      "train loss:1.14567167754\n",
      "=== epoch:269, train acc:0.626666666667, test acc:0.5204 ===\n",
      "train loss:1.41840344893\n",
      "train loss:1.24772992981\n",
      "train loss:1.17518006655\n",
      "=== epoch:270, train acc:0.643333333333, test acc:0.5269 ===\n",
      "train loss:1.09156189272\n",
      "train loss:1.10249085654\n",
      "train loss:1.12551676332\n",
      "=== epoch:271, train acc:0.643333333333, test acc:0.5297 ===\n",
      "train loss:1.29675668657\n",
      "train loss:1.12261245109\n",
      "train loss:1.18279902974\n",
      "=== epoch:272, train acc:0.646666666667, test acc:0.5318 ===\n",
      "train loss:1.0567911548\n",
      "train loss:1.26601586169\n",
      "train loss:1.22605385629\n",
      "=== epoch:273, train acc:0.64, test acc:0.533 ===\n",
      "train loss:1.1186247547\n",
      "train loss:1.27870078002\n",
      "train loss:1.09715649933\n",
      "=== epoch:274, train acc:0.646666666667, test acc:0.5347 ===\n",
      "train loss:1.19101075877\n",
      "train loss:1.23937283433\n",
      "train loss:1.07552253493\n",
      "=== epoch:275, train acc:0.646666666667, test acc:0.5336 ===\n",
      "train loss:1.21177467765\n",
      "train loss:1.13895445374\n",
      "train loss:1.26040366755\n",
      "=== epoch:276, train acc:0.653333333333, test acc:0.5378 ===\n",
      "train loss:1.04043923997\n",
      "train loss:1.18350798637\n",
      "train loss:1.30523529334\n",
      "=== epoch:277, train acc:0.656666666667, test acc:0.5371 ===\n",
      "train loss:1.04758158741\n",
      "train loss:1.29691804461\n",
      "train loss:1.09917980063\n",
      "=== epoch:278, train acc:0.65, test acc:0.5372 ===\n",
      "train loss:1.03286113671\n",
      "train loss:1.15618480647\n",
      "train loss:1.14148862374\n",
      "=== epoch:279, train acc:0.656666666667, test acc:0.5394 ===\n",
      "train loss:1.21868995865\n",
      "train loss:1.14858260227\n",
      "train loss:1.17218019281\n",
      "=== epoch:280, train acc:0.653333333333, test acc:0.5402 ===\n",
      "train loss:1.11507926823\n",
      "train loss:1.21857362383\n",
      "train loss:1.16736415701\n",
      "=== epoch:281, train acc:0.666666666667, test acc:0.5441 ===\n",
      "train loss:1.08740310498\n",
      "train loss:1.17463413306\n",
      "train loss:1.11912522538\n",
      "=== epoch:282, train acc:0.666666666667, test acc:0.5456 ===\n",
      "train loss:0.980300129946\n",
      "train loss:1.16906726375\n",
      "train loss:1.02293352764\n",
      "=== epoch:283, train acc:0.663333333333, test acc:0.5443 ===\n",
      "train loss:1.25608068816\n",
      "train loss:1.11665569034\n",
      "train loss:1.24982722622\n",
      "=== epoch:284, train acc:0.666666666667, test acc:0.5459 ===\n",
      "train loss:1.031610816\n",
      "train loss:1.15983590549\n",
      "train loss:1.24183934394\n",
      "=== epoch:285, train acc:0.66, test acc:0.5455 ===\n",
      "train loss:1.08725614799\n",
      "train loss:1.04809252111\n",
      "train loss:0.928660949374\n",
      "=== epoch:286, train acc:0.666666666667, test acc:0.5466 ===\n",
      "train loss:1.09018251822\n",
      "train loss:1.10996704626\n",
      "train loss:1.04947807815\n",
      "=== epoch:287, train acc:0.666666666667, test acc:0.5451 ===\n",
      "train loss:0.996241876765\n",
      "train loss:1.13712665939\n",
      "train loss:1.06823719894\n",
      "=== epoch:288, train acc:0.663333333333, test acc:0.549 ===\n",
      "train loss:1.07196881567\n",
      "train loss:1.14387789673\n",
      "train loss:0.949992278419\n",
      "=== epoch:289, train acc:0.666666666667, test acc:0.5519 ===\n",
      "train loss:1.08456735113\n",
      "train loss:1.06497326717\n",
      "train loss:1.06742597559\n",
      "=== epoch:290, train acc:0.666666666667, test acc:0.5574 ===\n",
      "train loss:1.08322055201\n",
      "train loss:1.00000986869\n",
      "train loss:0.996146152635\n",
      "=== epoch:291, train acc:0.663333333333, test acc:0.5541 ===\n",
      "train loss:1.16204672582\n",
      "train loss:1.00935081234\n",
      "train loss:1.08169585043\n",
      "=== epoch:292, train acc:0.67, test acc:0.5535 ===\n",
      "train loss:1.14255628693\n",
      "train loss:1.07574500616\n",
      "train loss:1.1887496957\n",
      "=== epoch:293, train acc:0.673333333333, test acc:0.5591 ===\n",
      "train loss:1.0922373348\n",
      "train loss:1.05029698546\n",
      "train loss:1.08691971388\n",
      "=== epoch:294, train acc:0.676666666667, test acc:0.5621 ===\n",
      "train loss:0.935937118763\n",
      "train loss:1.06152452677\n",
      "train loss:0.990497812812\n",
      "=== epoch:295, train acc:0.673333333333, test acc:0.5601 ===\n",
      "train loss:1.06150608767\n",
      "train loss:0.951374187396\n",
      "train loss:1.08763444955\n",
      "=== epoch:296, train acc:0.683333333333, test acc:0.5642 ===\n",
      "train loss:1.05622913184\n",
      "train loss:0.938808345355\n",
      "train loss:1.06780427591\n",
      "=== epoch:297, train acc:0.683333333333, test acc:0.5642 ===\n",
      "train loss:1.05411120555\n",
      "train loss:0.969766227054\n",
      "train loss:1.02734719891\n",
      "=== epoch:298, train acc:0.676666666667, test acc:0.5629 ===\n",
      "train loss:0.995926764411\n",
      "train loss:0.952095721175\n",
      "train loss:1.01746652002\n",
      "=== epoch:299, train acc:0.683333333333, test acc:0.5656 ===\n",
      "train loss:0.894436163151\n",
      "train loss:1.02543616533\n",
      "train loss:0.966598184102\n",
      "=== epoch:300, train acc:0.683333333333, test acc:0.5637 ===\n",
      "train loss:1.14907025705\n",
      "train loss:0.944718058806\n",
      "train loss:1.12513648682\n",
      "=== epoch:301, train acc:0.68, test acc:0.5667 ===\n",
      "train loss:1.01520324271\n",
      "train loss:1.03147185953\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5672\n"
     ]
    }
   ],
   "source": [
    "use_dropout = True\n",
    "dropout_ratio = 0.2\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784,\n",
    "                              hidden_size_list=[100 for _ in range(6)],\n",
    "                              output_size=10,\n",
    "                              use_dropout=use_dropout,\n",
    "                              dropout_ration=dropout_ratio,\n",
    ")\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNUalh27UAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6eca0a6828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='o', label='test', markevery=10)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
