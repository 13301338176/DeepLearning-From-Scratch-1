{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# if you want to reduce training time, apply following lines.\n",
    "x_train = x_train[:5000]\n",
    "t_train = t_train[:5000]\n",
    "x_test = x_test[:1000]\n",
    "t_test = t_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.29957183819\n",
      "=== epoch:1, train acc:0.234, test acc:0.257 ===\n",
      "train loss:2.29838042254\n",
      "train loss:2.29395918775\n",
      "train loss:2.28833944574\n",
      "train loss:2.2783682081\n",
      "train loss:2.27203745661\n",
      "train loss:2.25046600979\n",
      "train loss:2.24314455165\n",
      "train loss:2.21708943913\n",
      "train loss:2.18590066554\n",
      "train loss:2.16803786507\n",
      "train loss:2.09848443506\n",
      "train loss:2.06027800434\n",
      "train loss:2.07157467195\n",
      "train loss:1.9948872263\n",
      "train loss:1.922884856\n",
      "train loss:1.80853033038\n",
      "train loss:1.76928262874\n",
      "train loss:1.71092498296\n",
      "train loss:1.57342054075\n",
      "train loss:1.4666078812\n",
      "train loss:1.49833108963\n",
      "train loss:1.45881353191\n",
      "train loss:1.27883730494\n",
      "train loss:1.23418993201\n",
      "train loss:1.08247939635\n",
      "train loss:1.10240323576\n",
      "train loss:1.09628866995\n",
      "train loss:1.01508291258\n",
      "train loss:0.816426814753\n",
      "train loss:1.00149946745\n",
      "train loss:0.876311900125\n",
      "train loss:0.899811339777\n",
      "train loss:0.770744647908\n",
      "train loss:0.727124524347\n",
      "train loss:0.669733291934\n",
      "train loss:0.775347552755\n",
      "train loss:0.617950493041\n",
      "train loss:0.648753745592\n",
      "train loss:0.689828625144\n",
      "train loss:0.61082492799\n",
      "train loss:0.567789809645\n",
      "train loss:0.558674618166\n",
      "train loss:0.528097720457\n",
      "train loss:0.788863036662\n",
      "train loss:0.517175455636\n",
      "train loss:0.616508714706\n",
      "train loss:0.491843884602\n",
      "train loss:0.532806405469\n",
      "train loss:0.711831596174\n",
      "train loss:0.676814382528\n",
      "=== epoch:2, train acc:0.786, test acc:0.768 ===\n",
      "train loss:0.59596835313\n",
      "train loss:0.638624908671\n",
      "train loss:0.616810569668\n",
      "train loss:0.629786192788\n",
      "train loss:0.527608998961\n",
      "train loss:0.466254601176\n",
      "train loss:0.470561738168\n",
      "train loss:0.424124832774\n",
      "train loss:0.632355866713\n",
      "train loss:0.626958754661\n",
      "train loss:0.416998178962\n",
      "train loss:0.447373763241\n",
      "train loss:0.371820752228\n",
      "train loss:0.373148520118\n",
      "train loss:0.381015180887\n",
      "train loss:0.380479730974\n",
      "train loss:0.518380447083\n",
      "train loss:0.474019701009\n",
      "train loss:0.446547945747\n",
      "train loss:0.263253265564\n",
      "train loss:0.35726760148\n",
      "train loss:0.367511435235\n",
      "train loss:0.341182677615\n",
      "train loss:0.399341815538\n",
      "train loss:0.412428194838\n",
      "train loss:0.389203228555\n",
      "train loss:0.419905604837\n",
      "train loss:0.695198545479\n",
      "train loss:0.351690555267\n",
      "train loss:0.555525271451\n",
      "train loss:0.452163258424\n",
      "train loss:0.316824468481\n",
      "train loss:0.462890701353\n",
      "train loss:0.350362921948\n",
      "train loss:0.323520782133\n",
      "train loss:0.62759275407\n",
      "train loss:0.318022817392\n",
      "train loss:0.32740446286\n",
      "train loss:0.414430756135\n",
      "train loss:0.332071521453\n",
      "train loss:0.774466572011\n",
      "train loss:0.586411818527\n",
      "train loss:0.346905721682\n",
      "train loss:0.298270548124\n",
      "train loss:0.287328697321\n",
      "train loss:0.324842362119\n",
      "train loss:0.433633443246\n",
      "train loss:0.361952045967\n",
      "train loss:0.313907320985\n",
      "train loss:0.51469680784\n",
      "=== epoch:3, train acc:0.882, test acc:0.85 ===\n",
      "train loss:0.296143680946\n",
      "train loss:0.493376872665\n",
      "train loss:0.236217075658\n",
      "train loss:0.305296094152\n",
      "train loss:0.39051246996\n",
      "train loss:0.33764944485\n",
      "train loss:0.231789796352\n",
      "train loss:0.394131370634\n",
      "train loss:0.233017626559\n",
      "train loss:0.327774009904\n",
      "train loss:0.438262933527\n",
      "train loss:0.283001863354\n",
      "train loss:0.328273526234\n",
      "train loss:0.248230173955\n",
      "train loss:0.307993904268\n",
      "train loss:0.309451628185\n",
      "train loss:0.269005628553\n",
      "train loss:0.340394348092\n",
      "train loss:0.328853320034\n",
      "train loss:0.271667430428\n",
      "train loss:0.194054869521\n",
      "train loss:0.228516747752\n",
      "train loss:0.231082872935\n",
      "train loss:0.383563803956\n",
      "train loss:0.226001666756\n",
      "train loss:0.223119668032\n",
      "train loss:0.271517778392\n",
      "train loss:0.257888495717\n",
      "train loss:0.299570647561\n",
      "train loss:0.336382380993\n",
      "train loss:0.310727832624\n",
      "train loss:0.288304543421\n",
      "train loss:0.370249422422\n",
      "train loss:0.26882082757\n",
      "train loss:0.199885168595\n",
      "train loss:0.292264779004\n",
      "train loss:0.291691297039\n",
      "train loss:0.357538076767\n",
      "train loss:0.447289101271\n",
      "train loss:0.429082078434\n",
      "train loss:0.320503343883\n",
      "train loss:0.368926346988\n",
      "train loss:0.224444189591\n",
      "train loss:0.382966785843\n",
      "train loss:0.44387088254\n",
      "train loss:0.447453441999\n",
      "train loss:0.311589539029\n",
      "train loss:0.297440377919\n",
      "train loss:0.283060729171\n",
      "train loss:0.35943392548\n",
      "=== epoch:4, train acc:0.89, test acc:0.873 ===\n",
      "train loss:0.334450574105\n",
      "train loss:0.288025003753\n",
      "train loss:0.295619933876\n",
      "train loss:0.19490506183\n",
      "train loss:0.300186710002\n",
      "train loss:0.344232836103\n",
      "train loss:0.441632197456\n",
      "train loss:0.298838178087\n",
      "train loss:0.273613894272\n",
      "train loss:0.363673221978\n",
      "train loss:0.226275648074\n",
      "train loss:0.275352113396\n",
      "train loss:0.319888543632\n",
      "train loss:0.294591871135\n",
      "train loss:0.227641629531\n",
      "train loss:0.195814430529\n",
      "train loss:0.378285665328\n",
      "train loss:0.524283971619\n",
      "train loss:0.307872689797\n",
      "train loss:0.220801339657\n",
      "train loss:0.265210730515\n",
      "train loss:0.243580631213\n",
      "train loss:0.351303394359\n",
      "train loss:0.406289995659\n",
      "train loss:0.261008414627\n",
      "train loss:0.297207363492\n",
      "train loss:0.280839778583\n",
      "train loss:0.209360766677\n",
      "train loss:0.280543672676\n",
      "train loss:0.275847265045\n",
      "train loss:0.290458347072\n",
      "train loss:0.232673211175\n",
      "train loss:0.407809021125\n",
      "train loss:0.277277270218\n",
      "train loss:0.434431542107\n",
      "train loss:0.24905849769\n",
      "train loss:0.205062198548\n",
      "train loss:0.36334706724\n",
      "train loss:0.216645463334\n",
      "train loss:0.265322948998\n",
      "train loss:0.222285303327\n",
      "train loss:0.4103969646\n",
      "train loss:0.29968748281\n",
      "train loss:0.344859633734\n",
      "train loss:0.272535686069\n",
      "train loss:0.229835563296\n",
      "train loss:0.26591359716\n",
      "train loss:0.214760075407\n",
      "train loss:0.135771785626\n",
      "train loss:0.38917891172\n",
      "=== epoch:5, train acc:0.909, test acc:0.888 ===\n",
      "train loss:0.211346319499\n",
      "train loss:0.298924029709\n",
      "train loss:0.284172424142\n",
      "train loss:0.277768844646\n",
      "train loss:0.211333884317\n",
      "train loss:0.214262438497\n",
      "train loss:0.237394499095\n",
      "train loss:0.228341645077\n",
      "train loss:0.163530380576\n",
      "train loss:0.188922714529\n",
      "train loss:0.340448238571\n",
      "train loss:0.166273620502\n",
      "train loss:0.302739616183\n",
      "train loss:0.26138897974\n",
      "train loss:0.149698089303\n",
      "train loss:0.290453748767\n",
      "train loss:0.31987124825\n",
      "train loss:0.190519735419\n",
      "train loss:0.26928535782\n",
      "train loss:0.183847818453\n",
      "train loss:0.240636790748\n",
      "train loss:0.261339213696\n",
      "train loss:0.460308844265\n",
      "train loss:0.190567707627\n",
      "train loss:0.292159791725\n",
      "train loss:0.14230445743\n",
      "train loss:0.34031773901\n",
      "train loss:0.382912140165\n",
      "train loss:0.261938533553\n",
      "train loss:0.255551732729\n",
      "train loss:0.203741821167\n",
      "train loss:0.294779928609\n",
      "train loss:0.203658773295\n",
      "train loss:0.348149738727\n",
      "train loss:0.174956272733\n",
      "train loss:0.225999472632\n",
      "train loss:0.281165286221\n",
      "train loss:0.32617720903\n",
      "train loss:0.193319749814\n",
      "train loss:0.187601000419\n",
      "train loss:0.279307777742\n",
      "train loss:0.094254464166\n",
      "train loss:0.181311990796\n",
      "train loss:0.21366369533\n",
      "train loss:0.210131507944\n",
      "train loss:0.276094618271\n",
      "train loss:0.20249253068\n",
      "train loss:0.339140169998\n",
      "train loss:0.216338462042\n",
      "train loss:0.313401517297\n",
      "=== epoch:6, train acc:0.912, test acc:0.884 ===\n",
      "train loss:0.304498220101\n",
      "train loss:0.151153722718\n",
      "train loss:0.266830423646\n",
      "train loss:0.194638521456\n",
      "train loss:0.212904960186\n",
      "train loss:0.153445246886\n",
      "train loss:0.18996376894\n",
      "train loss:0.262234093806\n",
      "train loss:0.128210358843\n",
      "train loss:0.247704760518\n",
      "train loss:0.171079567354\n",
      "train loss:0.232879893836\n",
      "train loss:0.295716748343\n",
      "train loss:0.198894957207\n",
      "train loss:0.228162195761\n",
      "train loss:0.220383418552\n",
      "train loss:0.166821333395\n",
      "train loss:0.242785748676\n",
      "train loss:0.190495772671\n",
      "train loss:0.259703972096\n",
      "train loss:0.230958481944\n",
      "train loss:0.222137323094\n",
      "train loss:0.171612022989\n",
      "train loss:0.199900714464\n",
      "train loss:0.141469073048\n",
      "train loss:0.138432005026\n",
      "train loss:0.183280670807\n",
      "train loss:0.260144297785\n",
      "train loss:0.142171571019\n",
      "train loss:0.113307891012\n",
      "train loss:0.0723403878309\n",
      "train loss:0.177596280756\n",
      "train loss:0.35764932409\n",
      "train loss:0.284678183309\n",
      "train loss:0.175460685472\n",
      "train loss:0.241665458605\n",
      "train loss:0.0902582251267\n",
      "train loss:0.258629943299\n",
      "train loss:0.16881079311\n",
      "train loss:0.178722087406\n",
      "train loss:0.242965410615\n",
      "train loss:0.2265980209\n",
      "train loss:0.173722774203\n",
      "train loss:0.21182065285\n",
      "train loss:0.177728702066\n",
      "train loss:0.275877254683\n",
      "train loss:0.231220849214\n",
      "train loss:0.171863211439\n",
      "train loss:0.179353094301\n",
      "train loss:0.160906279972\n",
      "=== epoch:7, train acc:0.93, test acc:0.915 ===\n",
      "train loss:0.155252087566\n",
      "train loss:0.212394187748\n",
      "train loss:0.145951020367\n",
      "train loss:0.165157676524\n",
      "train loss:0.219788986037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.189596991677\n",
      "train loss:0.197514735806\n",
      "train loss:0.109428448507\n",
      "train loss:0.221289533397\n",
      "train loss:0.197520450453\n",
      "train loss:0.313527515049\n",
      "train loss:0.14056433835\n",
      "train loss:0.243372716851\n",
      "train loss:0.168406663457\n",
      "train loss:0.320390691761\n",
      "train loss:0.135221008693\n",
      "train loss:0.176878734167\n",
      "train loss:0.239659226898\n",
      "train loss:0.145008723926\n",
      "train loss:0.113484340116\n",
      "train loss:0.122366780723\n",
      "train loss:0.147841860816\n",
      "train loss:0.202279424311\n",
      "train loss:0.198308844942\n",
      "train loss:0.144567191455\n",
      "train loss:0.138058619323\n",
      "train loss:0.121593781285\n",
      "train loss:0.261313217275\n",
      "train loss:0.155398738083\n",
      "train loss:0.247896854243\n",
      "train loss:0.147380103627\n",
      "train loss:0.0942854361923\n",
      "train loss:0.26449844267\n",
      "train loss:0.101495977995\n",
      "train loss:0.180019980922\n",
      "train loss:0.292912953402\n",
      "train loss:0.19326571376\n",
      "train loss:0.189200807229\n",
      "train loss:0.0960903067892\n",
      "train loss:0.189561132678\n",
      "train loss:0.287790973364\n",
      "train loss:0.199036838171\n",
      "train loss:0.166183613476\n",
      "train loss:0.121761063144\n",
      "train loss:0.210273701776\n",
      "train loss:0.154056720722\n",
      "train loss:0.229744146914\n",
      "train loss:0.222652978369\n",
      "train loss:0.0862554310701\n",
      "train loss:0.160341305305\n",
      "=== epoch:8, train acc:0.943, test acc:0.918 ===\n",
      "train loss:0.203399611636\n",
      "train loss:0.172782985869\n",
      "train loss:0.274225758485\n",
      "train loss:0.114369113053\n",
      "train loss:0.181414167415\n",
      "train loss:0.159028508396\n",
      "train loss:0.187010468949\n",
      "train loss:0.187497408145\n",
      "train loss:0.215490028034\n",
      "train loss:0.14714466507\n",
      "train loss:0.184509590434\n",
      "train loss:0.173159291011\n",
      "train loss:0.178903316859\n",
      "train loss:0.0982559330813\n",
      "train loss:0.128063796702\n",
      "train loss:0.267779971168\n",
      "train loss:0.307801849309\n",
      "train loss:0.121923122864\n",
      "train loss:0.133154202914\n",
      "train loss:0.200680995749\n",
      "train loss:0.196132939032\n",
      "train loss:0.0969051713139\n",
      "train loss:0.117956885672\n",
      "train loss:0.124886317316\n",
      "train loss:0.113559572194\n",
      "train loss:0.164605922025\n",
      "train loss:0.192450321598\n",
      "train loss:0.0858655709581\n",
      "train loss:0.15492001836\n",
      "train loss:0.108289862451\n",
      "train loss:0.192003386324\n",
      "train loss:0.10346670665\n",
      "train loss:0.0850361694838\n",
      "train loss:0.218438592648\n",
      "train loss:0.268279216031\n",
      "train loss:0.190060509137\n",
      "train loss:0.203244895057\n",
      "train loss:0.139573924614\n",
      "train loss:0.0804853030804\n",
      "train loss:0.223976561305\n",
      "train loss:0.186075234849\n",
      "train loss:0.170315124228\n",
      "train loss:0.201357197621\n",
      "train loss:0.107395945559\n",
      "train loss:0.0995059043883\n",
      "train loss:0.101961368465\n",
      "train loss:0.142329049663\n",
      "train loss:0.12822049977\n",
      "train loss:0.134422187498\n",
      "train loss:0.0705503930139\n",
      "=== epoch:9, train acc:0.946, test acc:0.926 ===\n",
      "train loss:0.134385268678\n",
      "train loss:0.144069888514\n",
      "train loss:0.195857948104\n",
      "train loss:0.211258226901\n",
      "train loss:0.179293675395\n",
      "train loss:0.211899047502\n",
      "train loss:0.123197707047\n",
      "train loss:0.0654466254638\n",
      "train loss:0.171149512771\n",
      "train loss:0.26165618651\n",
      "train loss:0.217494599384\n",
      "train loss:0.116894428176\n",
      "train loss:0.0934491341966\n",
      "train loss:0.169121295957\n",
      "train loss:0.185978963872\n",
      "train loss:0.126601486748\n",
      "train loss:0.190288626259\n",
      "train loss:0.119223130313\n",
      "train loss:0.0654876547134\n",
      "train loss:0.101499544879\n",
      "train loss:0.0780748353845\n",
      "train loss:0.081439960133\n",
      "train loss:0.102235167456\n",
      "train loss:0.0829990985267\n",
      "train loss:0.180874615907\n",
      "train loss:0.114272249614\n",
      "train loss:0.154041471368\n",
      "train loss:0.0693516338484\n",
      "train loss:0.0796589411548\n",
      "train loss:0.0803177481621\n",
      "train loss:0.0697588437846\n",
      "train loss:0.0913457581461\n",
      "train loss:0.133094506755\n",
      "train loss:0.110476699368\n",
      "train loss:0.121324780772\n",
      "train loss:0.17729230475\n",
      "train loss:0.152253439467\n",
      "train loss:0.0869570469186\n",
      "train loss:0.0854036065367\n",
      "train loss:0.100960606719\n",
      "train loss:0.116709875028\n",
      "train loss:0.160408551121\n",
      "train loss:0.107440801958\n",
      "train loss:0.157367161785\n",
      "train loss:0.0918534846814\n",
      "train loss:0.205891768185\n",
      "train loss:0.106116316442\n",
      "train loss:0.066157472826\n",
      "train loss:0.0829977896694\n",
      "train loss:0.119804898013\n",
      "=== epoch:10, train acc:0.956, test acc:0.925 ===\n",
      "train loss:0.109500111548\n",
      "train loss:0.176248493015\n",
      "train loss:0.17739353378\n",
      "train loss:0.12533820834\n",
      "train loss:0.132638113347\n",
      "train loss:0.12070857513\n",
      "train loss:0.0740272325507\n",
      "train loss:0.10962107366\n",
      "train loss:0.159563399292\n",
      "train loss:0.150296755783\n",
      "train loss:0.112384397976\n",
      "train loss:0.112640874014\n",
      "train loss:0.124612675466\n",
      "train loss:0.0868031351226\n",
      "train loss:0.0974014105159\n",
      "train loss:0.0770277749205\n",
      "train loss:0.112486890328\n",
      "train loss:0.0807598742694\n",
      "train loss:0.0741870894948\n",
      "train loss:0.163819647671\n",
      "train loss:0.24581114021\n",
      "train loss:0.100062088473\n",
      "train loss:0.127776396914\n",
      "train loss:0.104525843093\n",
      "train loss:0.0629670765803\n",
      "train loss:0.0315714162677\n",
      "train loss:0.0990629510017\n",
      "train loss:0.0912318368268\n",
      "train loss:0.0862161362057\n",
      "train loss:0.0996889749487\n",
      "train loss:0.112884990983\n",
      "train loss:0.141672000096\n",
      "train loss:0.061629718989\n",
      "train loss:0.0712376030609\n",
      "train loss:0.101345281679\n",
      "train loss:0.168683429076\n",
      "train loss:0.182399604827\n",
      "train loss:0.0912128786264\n",
      "train loss:0.182985076468\n",
      "train loss:0.0568280349932\n",
      "train loss:0.0907948888508\n",
      "train loss:0.063829444732\n",
      "train loss:0.0953157559185\n",
      "train loss:0.144502325394\n",
      "train loss:0.0787523029117\n",
      "train loss:0.0950177995109\n",
      "train loss:0.0708429979916\n",
      "train loss:0.16296526641\n",
      "train loss:0.0823671706172\n",
      "train loss:0.0704431790643\n",
      "=== epoch:11, train acc:0.961, test acc:0.938 ===\n",
      "train loss:0.056737763581\n",
      "train loss:0.0703747455623\n",
      "train loss:0.0891786639309\n",
      "train loss:0.149475799227\n",
      "train loss:0.0785770277023\n",
      "train loss:0.0975643520714\n",
      "train loss:0.0595160081695\n",
      "train loss:0.101196248076\n",
      "train loss:0.156548484946\n",
      "train loss:0.104630969394\n",
      "train loss:0.0556002876832\n",
      "train loss:0.0983052242767\n",
      "train loss:0.136956593582\n",
      "train loss:0.0996235289868\n",
      "train loss:0.201971767991\n",
      "train loss:0.0981349625194\n",
      "train loss:0.0750367131635\n",
      "train loss:0.0939971760066\n",
      "train loss:0.0605702678485\n",
      "train loss:0.0458351455113\n",
      "train loss:0.112946744972\n",
      "train loss:0.119419866535\n",
      "train loss:0.12011597313\n",
      "train loss:0.114094014983\n",
      "train loss:0.082351211289\n",
      "train loss:0.0958507815045\n",
      "train loss:0.0514862586058\n",
      "train loss:0.0961647454766\n",
      "train loss:0.0621612032302\n",
      "train loss:0.10259054365\n",
      "train loss:0.0606187698065\n",
      "train loss:0.147147138987\n",
      "train loss:0.153745365126\n",
      "train loss:0.0553287039516\n",
      "train loss:0.0933200377869\n",
      "train loss:0.121798638342\n",
      "train loss:0.129606688472\n",
      "train loss:0.0629304264808\n",
      "train loss:0.0697692996379\n",
      "train loss:0.119306320068\n",
      "train loss:0.127660562611\n",
      "train loss:0.0842967101076\n",
      "train loss:0.0774692511419\n",
      "train loss:0.102306105029\n",
      "train loss:0.0889410396785\n",
      "train loss:0.0515294283798\n",
      "train loss:0.18531527003\n",
      "train loss:0.05709234041\n",
      "train loss:0.069758801363\n",
      "train loss:0.0573727140836\n",
      "=== epoch:12, train acc:0.961, test acc:0.936 ===\n",
      "train loss:0.0937123434031\n",
      "train loss:0.024925534006\n",
      "train loss:0.114275401952\n",
      "train loss:0.0600953798256\n",
      "train loss:0.0815241216237\n",
      "train loss:0.0994625199443\n",
      "train loss:0.0632386272302\n",
      "train loss:0.0796560524336\n",
      "train loss:0.0280374638505\n",
      "train loss:0.0582766349028\n",
      "train loss:0.0528871792336\n",
      "train loss:0.10586651166\n",
      "train loss:0.0535444655015\n",
      "train loss:0.11987039691\n",
      "train loss:0.0803987300268\n",
      "train loss:0.0619154873337\n",
      "train loss:0.171205900196\n",
      "train loss:0.140343975424\n",
      "train loss:0.0509161404021\n",
      "train loss:0.152462944139\n",
      "train loss:0.0981692065948\n",
      "train loss:0.0778914162273\n",
      "train loss:0.0620481984979\n",
      "train loss:0.0790909305897\n",
      "train loss:0.112739713812\n",
      "train loss:0.150257344862\n",
      "train loss:0.0944074265142\n",
      "train loss:0.0776207445731\n",
      "train loss:0.0512506647927\n",
      "train loss:0.0482798562629\n",
      "train loss:0.10261308786\n",
      "train loss:0.067944879145\n",
      "train loss:0.0972214815365\n",
      "train loss:0.0635914859753\n",
      "train loss:0.0590262719893\n",
      "train loss:0.0531031039545\n",
      "train loss:0.09909800984\n",
      "train loss:0.0388292330991\n",
      "train loss:0.0926767528667\n",
      "train loss:0.04833307885\n",
      "train loss:0.0455898274429\n",
      "train loss:0.0543171444026\n",
      "train loss:0.0340637354811\n",
      "train loss:0.0988248977471\n",
      "train loss:0.0828853374649\n",
      "train loss:0.07394371682\n",
      "train loss:0.150863892665\n",
      "train loss:0.0499554470388\n",
      "train loss:0.10315010525\n",
      "train loss:0.0346076686417\n",
      "=== epoch:13, train acc:0.967, test acc:0.947 ===\n",
      "train loss:0.0543724382095\n",
      "train loss:0.118688225527\n",
      "train loss:0.136244713016\n",
      "train loss:0.0328766896761\n",
      "train loss:0.0488592782666\n",
      "train loss:0.0406053290706\n",
      "train loss:0.0776170669949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0781202612639\n",
      "train loss:0.0817569931219\n",
      "train loss:0.198411550569\n",
      "train loss:0.064963008272\n",
      "train loss:0.0913398798485\n",
      "train loss:0.109342613336\n",
      "train loss:0.0444279398087\n",
      "train loss:0.106448489115\n",
      "train loss:0.0796799443155\n",
      "train loss:0.0981539732331\n",
      "train loss:0.0889359227693\n",
      "train loss:0.161041010505\n",
      "train loss:0.0786957265285\n",
      "train loss:0.0610453312296\n",
      "train loss:0.0970339311676\n",
      "train loss:0.0358240362953\n",
      "train loss:0.0351758666177\n",
      "train loss:0.13372948048\n",
      "train loss:0.0730054139762\n",
      "train loss:0.0593631941706\n",
      "train loss:0.0524160645833\n",
      "train loss:0.0755507404004\n",
      "train loss:0.13236334323\n",
      "train loss:0.0733292198773\n",
      "train loss:0.0379882538154\n",
      "train loss:0.0640784668315\n",
      "train loss:0.107851955739\n",
      "train loss:0.102640341838\n",
      "train loss:0.0909160046118\n",
      "train loss:0.0417857041724\n",
      "train loss:0.084027997743\n",
      "train loss:0.118052919706\n",
      "train loss:0.110484175938\n",
      "train loss:0.0746464663762\n",
      "train loss:0.0629565595175\n",
      "train loss:0.0743297249695\n",
      "train loss:0.0875578691225\n",
      "train loss:0.0462568266277\n",
      "train loss:0.0969351046632\n",
      "train loss:0.0310960387037\n",
      "train loss:0.041092103128\n",
      "train loss:0.058973303941\n",
      "train loss:0.0197472713353\n",
      "=== epoch:14, train acc:0.977, test acc:0.948 ===\n",
      "train loss:0.0480274318527\n",
      "train loss:0.0593992327544\n",
      "train loss:0.0589658462759\n",
      "train loss:0.0482533585778\n",
      "train loss:0.0712081823533\n",
      "train loss:0.0405691789896\n",
      "train loss:0.030204838516\n",
      "train loss:0.0949397194103\n",
      "train loss:0.0986549681377\n",
      "train loss:0.0432860844246\n",
      "train loss:0.0229038174044\n",
      "train loss:0.0588079749091\n",
      "train loss:0.121548218874\n",
      "train loss:0.0674420752558\n",
      "train loss:0.0916689578284\n",
      "train loss:0.106469385882\n",
      "train loss:0.101373698879\n",
      "train loss:0.0988680470287\n",
      "train loss:0.0533812910459\n",
      "train loss:0.0279382651345\n",
      "train loss:0.0441819311096\n",
      "train loss:0.0454615700983\n",
      "train loss:0.024909944044\n",
      "train loss:0.018554677286\n",
      "train loss:0.0252352090083\n",
      "train loss:0.0795459387883\n",
      "train loss:0.0750748892272\n",
      "train loss:0.0868478869784\n",
      "train loss:0.0303100983191\n",
      "train loss:0.0427790320034\n",
      "train loss:0.0468985465237\n",
      "train loss:0.0574280783213\n",
      "train loss:0.0698348508124\n",
      "train loss:0.0587319476582\n",
      "train loss:0.0239102523405\n",
      "train loss:0.0280885618159\n",
      "train loss:0.0182650291609\n",
      "train loss:0.0355791762514\n",
      "train loss:0.106667560397\n",
      "train loss:0.0386932283117\n",
      "train loss:0.0185960631874\n",
      "train loss:0.0390005318904\n",
      "train loss:0.108048774441\n",
      "train loss:0.0339386363892\n",
      "train loss:0.015892344664\n",
      "train loss:0.0201026067599\n",
      "train loss:0.0282634851896\n",
      "train loss:0.0446812887061\n",
      "train loss:0.0563236611512\n",
      "train loss:0.0210388071285\n",
      "=== epoch:15, train acc:0.978, test acc:0.953 ===\n",
      "train loss:0.0313145974251\n",
      "train loss:0.0656756182897\n",
      "train loss:0.0629582029657\n",
      "train loss:0.0196372164337\n",
      "train loss:0.076537729149\n",
      "train loss:0.0494824655944\n",
      "train loss:0.0256941162609\n",
      "train loss:0.0433226176252\n",
      "train loss:0.070877858961\n",
      "train loss:0.027893190884\n",
      "train loss:0.0641689328307\n",
      "train loss:0.0478394621468\n",
      "train loss:0.0617275590225\n",
      "train loss:0.0426105298123\n",
      "train loss:0.0322534925407\n",
      "train loss:0.0502586225444\n",
      "train loss:0.0459930949338\n",
      "train loss:0.0725905164528\n",
      "train loss:0.0536400388099\n",
      "train loss:0.046592794329\n",
      "train loss:0.0464362158482\n",
      "train loss:0.0497410248006\n",
      "train loss:0.096315713805\n",
      "train loss:0.0354237544925\n",
      "train loss:0.103338768268\n",
      "train loss:0.0417580605522\n",
      "train loss:0.05291371088\n",
      "train loss:0.0323760110849\n",
      "train loss:0.0348397323579\n",
      "train loss:0.0485115098482\n",
      "train loss:0.0449647615085\n",
      "train loss:0.0323244196119\n",
      "train loss:0.0319921404462\n",
      "train loss:0.0919327631733\n",
      "train loss:0.0320477718146\n",
      "train loss:0.0435401059633\n",
      "train loss:0.0629824963063\n",
      "train loss:0.0228875430735\n",
      "train loss:0.0640897676848\n",
      "train loss:0.0534588208469\n",
      "train loss:0.0378229597691\n",
      "train loss:0.0316859258333\n",
      "train loss:0.0584501953924\n",
      "train loss:0.0283303009987\n",
      "train loss:0.0450217106008\n",
      "train loss:0.0505051434293\n",
      "train loss:0.0542174537337\n",
      "train loss:0.0603254910998\n",
      "train loss:0.0504974144547\n",
      "train loss:0.0480228846913\n",
      "=== epoch:16, train acc:0.975, test acc:0.94 ===\n",
      "train loss:0.0953153487503\n",
      "train loss:0.0637055901013\n",
      "train loss:0.0675451960236\n",
      "train loss:0.0286609400793\n",
      "train loss:0.100334242336\n",
      "train loss:0.0646046652324\n",
      "train loss:0.0510168815828\n",
      "train loss:0.0262253342353\n",
      "train loss:0.020452590296\n",
      "train loss:0.0528187229764\n",
      "train loss:0.0386694693437\n",
      "train loss:0.0689875826348\n",
      "train loss:0.0350604168934\n",
      "train loss:0.0478776871704\n",
      "train loss:0.075849878712\n",
      "train loss:0.0421582572563\n",
      "train loss:0.0101999832361\n",
      "train loss:0.0377713679343\n",
      "train loss:0.0184775186952\n",
      "train loss:0.0403804820812\n",
      "train loss:0.0517676750737\n",
      "train loss:0.0340914347294\n",
      "train loss:0.0551030631464\n",
      "train loss:0.0466466721631\n",
      "train loss:0.0281228357741\n",
      "train loss:0.0833619362826\n",
      "train loss:0.0345877101079\n",
      "train loss:0.0225261915722\n",
      "train loss:0.037103060943\n",
      "train loss:0.0597617587113\n",
      "train loss:0.0487848063383\n",
      "train loss:0.0344515279249\n",
      "train loss:0.0336706123333\n",
      "train loss:0.0413610588265\n",
      "train loss:0.0342153987072\n",
      "train loss:0.021674636528\n",
      "train loss:0.0375033986625\n",
      "train loss:0.045799391631\n",
      "train loss:0.0233626979024\n",
      "train loss:0.0217662180257\n",
      "train loss:0.107282992637\n",
      "train loss:0.0172144159878\n",
      "train loss:0.0233708464369\n",
      "train loss:0.0759273701817\n",
      "train loss:0.0390859248208\n",
      "train loss:0.0739649098997\n",
      "train loss:0.0142743102779\n",
      "train loss:0.0272566645397\n",
      "train loss:0.0145547271103\n",
      "train loss:0.0481062596605\n",
      "=== epoch:17, train acc:0.981, test acc:0.949 ===\n",
      "train loss:0.0141778054772\n",
      "train loss:0.060007967917\n",
      "train loss:0.0545242439121\n",
      "train loss:0.0406313224827\n",
      "train loss:0.0713930684503\n",
      "train loss:0.0571867375592\n",
      "train loss:0.0220924852803\n",
      "train loss:0.0792102174255\n",
      "train loss:0.0591904673595\n",
      "train loss:0.0281619363986\n",
      "train loss:0.0301680108437\n",
      "train loss:0.0249407443598\n",
      "train loss:0.0319654434259\n",
      "train loss:0.0627752731306\n",
      "train loss:0.0367280738306\n",
      "train loss:0.0384541032133\n",
      "train loss:0.0557181366652\n",
      "train loss:0.040700232143\n",
      "train loss:0.0287020048383\n",
      "train loss:0.0390873802626\n",
      "train loss:0.0490585342523\n",
      "train loss:0.0398084059439\n",
      "train loss:0.0673884457393\n",
      "train loss:0.0128503100167\n",
      "train loss:0.0320342158235\n",
      "train loss:0.0338538832214\n",
      "train loss:0.035993390329\n",
      "train loss:0.0199641169877\n",
      "train loss:0.0415996950198\n",
      "train loss:0.0194969457658\n",
      "train loss:0.0137409316069\n",
      "train loss:0.0313152929547\n",
      "train loss:0.0315900244439\n",
      "train loss:0.0466370963215\n",
      "train loss:0.0553832597021\n",
      "train loss:0.0208901835326\n",
      "train loss:0.0839509402002\n",
      "train loss:0.048978833836\n",
      "train loss:0.0513235862859\n",
      "train loss:0.0229562725525\n",
      "train loss:0.0234530728989\n",
      "train loss:0.0370286433865\n",
      "train loss:0.0208364267124\n",
      "train loss:0.037887466352\n",
      "train loss:0.0366214086602\n",
      "train loss:0.0398344119834\n",
      "train loss:0.0108948979565\n",
      "train loss:0.0321153524105\n",
      "train loss:0.0911956227415\n",
      "train loss:0.0785894115823\n",
      "=== epoch:18, train acc:0.988, test acc:0.949 ===\n",
      "train loss:0.0387713269273\n",
      "train loss:0.00761075537654\n",
      "train loss:0.0251817681488\n",
      "train loss:0.0454919299672\n",
      "train loss:0.0251503254414\n",
      "train loss:0.0405576931795\n",
      "train loss:0.05085288499\n",
      "train loss:0.0210308586928\n",
      "train loss:0.0388853404043\n",
      "train loss:0.0743429392536\n",
      "train loss:0.0524307138725\n",
      "train loss:0.0443808770607\n",
      "train loss:0.0182410285431\n",
      "train loss:0.0244732705879\n",
      "train loss:0.0210979649987\n",
      "train loss:0.0295132387665\n",
      "train loss:0.0165850279109\n",
      "train loss:0.0216376635892\n",
      "train loss:0.0207154093249\n",
      "train loss:0.0355962567665\n",
      "train loss:0.0408630133613\n",
      "train loss:0.0348019892361\n",
      "train loss:0.0452868235175\n",
      "train loss:0.0356992307156\n",
      "train loss:0.0252951072265\n",
      "train loss:0.0741236281875\n",
      "train loss:0.0333896215972\n",
      "train loss:0.0254744114843\n",
      "train loss:0.0118822413003\n",
      "train loss:0.0122337672165\n",
      "train loss:0.025069977248\n",
      "train loss:0.0166349091226\n",
      "train loss:0.0169729350988\n",
      "train loss:0.00908983092387\n",
      "train loss:0.01796681605\n",
      "train loss:0.0200417212253\n",
      "train loss:0.0221582467109\n",
      "train loss:0.0256530292031\n",
      "train loss:0.0565387644485\n",
      "train loss:0.0224200403133\n",
      "train loss:0.0181124813744\n",
      "train loss:0.055640525782\n",
      "train loss:0.027904443285\n",
      "train loss:0.011834206808\n",
      "train loss:0.0141711010459\n",
      "train loss:0.00966689348963\n",
      "train loss:0.0267864442148\n",
      "train loss:0.0214944670246\n",
      "train loss:0.0556931465244\n",
      "train loss:0.0226744664579\n",
      "=== epoch:19, train acc:0.992, test acc:0.955 ===\n",
      "train loss:0.016101443283\n",
      "train loss:0.0220076259739\n",
      "train loss:0.0669534270993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0441232834415\n",
      "train loss:0.0119639550654\n",
      "train loss:0.107826785861\n",
      "train loss:0.0246164895222\n",
      "train loss:0.0742906074618\n",
      "train loss:0.00840122134199\n",
      "train loss:0.0213558613776\n",
      "train loss:0.03194549787\n",
      "train loss:0.0261357711575\n",
      "train loss:0.0254104122759\n",
      "train loss:0.0145047919272\n",
      "train loss:0.0269161853487\n",
      "train loss:0.011207063775\n",
      "train loss:0.032638437354\n",
      "train loss:0.00923149134644\n",
      "train loss:0.0330225666286\n",
      "train loss:0.00483374418139\n",
      "train loss:0.0145952433888\n",
      "train loss:0.102306112678\n",
      "train loss:0.0188843340502\n",
      "train loss:0.0208067824118\n",
      "train loss:0.0299523800961\n",
      "train loss:0.053921814739\n",
      "train loss:0.0556639647121\n",
      "train loss:0.0240295494822\n",
      "train loss:0.0298310872267\n",
      "train loss:0.0255176482916\n",
      "train loss:0.0465170974988\n",
      "train loss:0.0147348704704\n",
      "train loss:0.0311729526528\n",
      "train loss:0.0185105703209\n",
      "train loss:0.0189435456733\n",
      "train loss:0.0153800399185\n",
      "train loss:0.0339980721977\n",
      "train loss:0.0214253941518\n",
      "train loss:0.0520613580716\n",
      "train loss:0.0604639833949\n",
      "train loss:0.0220599909344\n",
      "train loss:0.0174731042636\n",
      "train loss:0.0149979677377\n",
      "train loss:0.0172693153673\n",
      "train loss:0.0340322482268\n",
      "train loss:0.0267173320405\n",
      "train loss:0.0320650147302\n",
      "train loss:0.0123233942769\n",
      "train loss:0.0436854187952\n",
      "train loss:0.0363067556935\n",
      "=== epoch:20, train acc:0.991, test acc:0.951 ===\n",
      "train loss:0.0261492740358\n",
      "train loss:0.0404869352538\n",
      "train loss:0.0108774521037\n",
      "train loss:0.045897619726\n",
      "train loss:0.019861183772\n",
      "train loss:0.0209834492639\n",
      "train loss:0.0260408099193\n",
      "train loss:0.0362657820041\n",
      "train loss:0.0623225156762\n",
      "train loss:0.0107399947026\n",
      "train loss:0.0282652391866\n",
      "train loss:0.0358468791927\n",
      "train loss:0.0133375002235\n",
      "train loss:0.0157675591881\n",
      "train loss:0.0189378193585\n",
      "train loss:0.0317595239842\n",
      "train loss:0.0318082947129\n",
      "train loss:0.0102128601406\n",
      "train loss:0.0206427221371\n",
      "train loss:0.0273869409564\n",
      "train loss:0.024795643799\n",
      "train loss:0.0101202071358\n",
      "train loss:0.0171530639637\n",
      "train loss:0.0453202911982\n",
      "train loss:0.0224372276534\n",
      "train loss:0.0366939326732\n",
      "train loss:0.0222346717661\n",
      "train loss:0.0121564307194\n",
      "train loss:0.00553531805979\n",
      "train loss:0.0190473786542\n",
      "train loss:0.0937859793638\n",
      "train loss:0.020232907095\n",
      "train loss:0.0127016785858\n",
      "train loss:0.0147415858451\n",
      "train loss:0.0398342362771\n",
      "train loss:0.0154833895572\n",
      "train loss:0.0235546194357\n",
      "train loss:0.014438234458\n",
      "train loss:0.0147493639808\n",
      "train loss:0.020032518458\n",
      "train loss:0.0166251350833\n",
      "train loss:0.0443647390709\n",
      "train loss:0.00866963129764\n",
      "train loss:0.0251218308609\n",
      "train loss:0.0574303748227\n",
      "train loss:0.0271799521651\n",
      "train loss:0.0280333373538\n",
      "train loss:0.0155190887207\n",
      "train loss:0.00761784478944\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.955\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
    "                        conv_param={'filter_num': 30,\n",
    "                                    'filter_size': 5,\n",
    "                                    'pad': 0,\n",
    "                                    'stride': 1,\n",
    "                                   }\n",
    "                       )\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000,\n",
    "                 )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "network.save_params('params.pkl')\n",
    "print('Saved Network Parameters!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA8bccFyNWL+FAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7b73fce2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
